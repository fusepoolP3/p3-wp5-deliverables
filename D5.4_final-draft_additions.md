#D5.4 Report - Draft additions for final version

*CMSB 2015-12-15:*  
*The texts below are draft revisions to the D5.4 report, to complete the missing results sections. They need to be integrated into the final report, but I thought it easier to draft them separately here. Please edit these draft additions as you see fit.*

###Document History

| Ver. | Name | Date | Remark | | :---: | :--- | :---: | :--- | 
| v0.3 | Johannes Hercher, Milos Jovanovik, Carl Blakeley, Adrian Gschwend, Rupert Westenthaler | 18.12.2015 | Test result additions - Pre-review version | 
| v0.4 | Rupert Westenthaler, Johannes Hercher, Milos Jovanovik, Carl Blakeley, Adrian Gschwend  | 22.12.2015 | Test result additions - Reviewed version |

###Quality Assurance Review

####v0.1
* 1st reviewer: Jakob Frank (SRFG)
* 2nd reviewer: Luigi Selmi (BUAS)

####v0.3
* 1st reviewer: { TO DO }
* 2nd reviewer: { TO DO }

##Executive Summary

*Existing text:*

" â€¦ The seed dataset, B3Kat, contains approximately 26 million bibliographic records. The three core datasets combined total around 2 billion triples. We estimate that the resulting data corpus after enrichment will be approaching 3 billion triples."

*Following section to be added:*

###Key Findings###

*December 2015*

####Interlinking Quality####
**B3Kat Entity Linking**: The validation testing processed all B3Kat records without an assigned GND classification, approximately 143,000 in total, to link them with concepts from DBpedia, the STW, IPTC, and Social Sciences thesauri, and to automatically assign GND subject classification codes. Manual evaluation of the results indicates ~80% of the subject keywords and GND IDs assigned to a B3Kat record are "useful", demonstrating that the platform provides a viable automated subject indexing tool to support manual indexing by librarians. 

**GND Entity Linking**: 

* {TO DO?: Johannes/Rupert: Revise/complete summary of interlinking performed and results/conclusions}

####Platform Performance####

In addition to the 143,000 B3Kat records, 2 million composite GND description documents have been enriched on the platform. 

As FP3 transformers receive their input data as POST requests and return the transformation output, memory constraints dictate that large datasets must be chunked. Using Virtuoso as the platform LDP server, and a node.js client to concatenate 100 GND or B3Kat descriptions per document, a single transforming LDPC (TLDPC) has been loaded with ~1500 source Turtle documents and an equivalent number of transformation output documents, each of up to 500KB; however the document storage limit of a single TLDPC has not been established.

The transformation speed appears linear (O(n)), with the processing time of the last of the 2 million GNDs taking no longer than for the first GNDs. Upload and transformation of the corresponding 2000 source documents took of the order of {TO DO: XXX} hours, giving an indicative processing rate of {TO DO: YYY} documents/minute. Obviously this is highly dependent on the nature of the source data and transformations.

* { TO DO: Milos: Revision of above summary of transformation performance } 

####System Performance

* { TO DO: Adrian: Summary of system performance statistics }

---

*Section "Validation Results", currently empty, to be renamed "Validation Testing and Results" and the following added*

##Validation Testing and Results

*December 2015*

Section "Use Case Task Details" above outlined the envisaged processing steps for the validation use case. The use case as originally described focussed on enriching GNDs by using entity linking to link GNDs with matching thesauri concepts. The actual validation extended the scope of the interlinking to include enriching B3Kat records by linking them too with matching thesauri entries.

In effect, the revised validation workflow became:

* For GND interlinking: steps 1-5, steps 6-7
* For B3Kat interlinking: steps 6-7

Step 8 was not performed, as it was deemed unnecessary. The pre-existing links in the GND dataset to DBpedia were sufficient for the other validation tasks, viz. for assessing the feasibility of generating contextual links based in part on DBpedia properties and abstracts. Consequently it was decided that finding additional links was a low priority.

###GND Interlinking

#### Transformation Source Data - Preparation and Upload
Steps 1-5 provide base GND documents for enrichment and interlinking. Each base GND document is the output of a CONSTRUCT query capturing the contents of a GND graph, one per GND. The GND graph contains a composite description of a GND, combining the original master GND with additional GND subject data from DBpedia and co-occurring concepts from B3Kat (including the book's subject headings, classification codes, title and subtitle)

The individual GND graphs were populated using a series of SPARQL INSERTs, driven by a node.js script. Once populated, the GND documents to be transformed were CONSTRUCTed and POSTed to FP3 TLDPCs using a second node.js script, to materialize steps 6-7. Rather than uploading one Turtle document per GND, each document concatenated 100 GND graphs. Snapshots of the [SPARQL](https://fusepool.atlassian.net/browse/FP-340) and [scripts](https://github.com/fusepoolP3/p3-large-scale-script-js) used can be found in JIRA and GitHub, though the actual versions subsequently used differed slightly.

####Transformer Pipeline

Both the GND and B3Kat interlinking tasks used the [Literal Extraction Transformer](https://github.com/fusepoolP3/p3-literal-extraction-transformer) (port 8305) to provide plain text input to Stanbol for further analysis, via the [Fusepool Stanbol Launcher](https://github.com/fusepoolP3/p3-stanbol-launcher) (port 8304).

The transformer pipeline associated with the target TLDPC was as follows:

**Transformer pipeline:** `http://ulcs.fusepool.info:8305/?transformer=http://ulcs.fusepool.info:8304/transformers/chain/fuberlin-extraction&lit-pred=http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#isString&lit-pred=http://dbpedia.org/ontology/abstract&lit-pred=http://d-nb.info/standards/elementset/gnd#publication&lit-pred=http://d-nb.info/standards/elementset/gnd#biographicalOrHistoricalInformation&min-lit-len=1`

**Pipeline action:** Performs entity linking of GNDs to STW, IPTC and The Soz thesauri concepts using various textual properties comprising:

* broader category names from the GND 
* additional information from B3Kat and DNB:
  * dcterms:description, dcterms:alternative isbd:subtitle (all added as nif:isString properties)
* additional information from DBpedia (where the GND includes a link to DBpedia):
  * DBpedia subject labels and abstracts

###B3Kat Interlinking

#### Transformation Source Data - Preparation and Upload

No preparation of the B3Kat records was necessary. A slightly modified version of the node.js script used for the GND interlinking steps 6-7 was used to POST CONSTRUCTed B3Kat documents to a TLDPC, again combining 100 B3Kat records per uploaded document.

####Transformer Pipelines

Two transformer pipelines were used, each linked to a separate TLDPC:

##### Thesauri Concept Linking Pipeline

**Transformer pipeline:** `http://ulcs.fusepool.info:8305/?transformer=http://ulcs.fusepool.info:8304/transformers/chain/fuberlin-extraction&lit-pred=http://purl.org/dc/elements/1.1/subject&lit-pred=http://purl.org/dc/elements/1.1/title&lit-pred=http://iflastandards.info/ns/isbd/elements/P1006&min-lit-len=1`

**Pipeline action:** Performs entity linking against STW, IPTC and The Soz thesauri concepts using the dc:subject, dc:title and isbd:hasOtherTitleInformation (P1006) properties as the source of literal texts for analysis. (See for example: <https://lod.b3kat.de/page/title/BV023347895>)

#####B3Kat Subject GND Linking Pipeline

**Transformer pipeline:** `http://ulcs.fusepool.info:8305/?transformer=http://ulcs.fusepool.info:8304/transformers/chain/gnd-plain-linking&lit-pred=http://purl.org/dc/elements/1.1/subject&min-lit-len=1`

**Pipeline action:** Performs plain entity linking of B3Kat records to GND concepts using a B3Kat record's dc:subject property as the source of literal texts for analysis.

###Supporting Stanbol Enhancement Chains

Two underlying Stanbol enhancement chains performed the entity linking, chains "gnd-plain-linking" and "fuberlin-extraction".

The *gnd-plain-linking* enhancement chain consists of 6 engines:

* LanguageDetectionEnhancementEngine
* OpenNlpSentenceDetectionEngine
* OpenNlpTokenizerEngine
* FstLinkingEngine - 1 instance for plain linking against GND
* TextAnnotationsNewModelEngine
* Fise2FamEngine

The *fuberlin-extraction* enhancement chain comprises 8 engines:

* TikaEngine
* LanguageDetectionEnhancementEngine
* CustomNERModelEnhancementEngine
* FstLinkingEngine - 3 instances: for plain linking against STW, IPTC and The Soz
* TextAnnotationsNewModelEngine
* Fise2FamEngine

As the texts for the LKC are not full sentences, the FST Linking Engine was chosen as the most suitable of the available Stanbol entity linking engines - in "plain linking" mode, it can link against all the words in a supplied text. SOLR indexes were created for each of the custom vocabularies being linked against: the STW Thesaurus for Economics, IPTC and the Soz (GESIS). See Appendix B for an overview of the Stanbol Content Enhancer features relevant to the LKC use case.

Although it was originally envisaged that dataTXT would form part of the LKC enhancement chains (as depicted in Appendix B) for linking against DBpedia, this was unnecessary for the actual validation and dataTxt was not used.

### Interlinking Quality Evaluation

* { TO DO: Johannes / Rupert }

#### GND Interlinking

 * { TO DO: Johannes}
   * Details of handpicked GNDs

#### B3Kat Interlinking

For interlinking analysis, 250 handpicked B3Kat records were used:

  * 150 with no URI in dcterms:subject
  * 100 with a URI in dcterms:subject and text in dc:subject

 * { TO DO: Johannes}

#### GND Keyword Clustering

 * { TO DO: Johannes}

### Platform Stress Testing

To assess how the platform copes with data volume, the fuberlin-extraction transformer pipeline was linked to 10 TLDPCs. A total of 2 million GND documents were uploaded, 2000 documents per TLDPC, 100 GND graphs per document. 

In addition, ~1430 B3Kat documents were posted to each of two TLDPCs, one linked to the fuberlin-extraction pipeline, the other to the gnd-plain-linking chain, with each document containing 100 B3Kat graphs. The ~143,000 B3Kat records uploaded comprised the subset of the 26 million B3Kat records with a dc:subject property and German language tag, as required by the enhancement chains.

* { TO DO: Milos: Performance figures & conclusions. Average source document size KB/triple counts etc. }
* Processing 2 million GNDs - initial runs:
  * Took ~140 hours for steps 1-5
  * Steps 6-8 took < 24 hours

### System Performance Statistics

* { TO DO: Adrian: methods, collectd figures etc}