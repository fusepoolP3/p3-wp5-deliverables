#Large-scale Technical Validation of Fusepool P3 Platform 
###Deliverable D5.4

![enter image description here](https://avatars0.githubusercontent.com/u/5859504?v=3&s=200)

[TOC]

----------

#### Document History ####

| Ver. | Name | Date | Remark |
| :---: | :--- | :---: | :--- |
| v0.0 | Carl Blakeley, Milos Jovanovik | 08.06.2015 | Initial draft |
| v0.1 | Milos Jovanovik, Carl Blakeley, Adrian Gschwend, Reto Gmür, Rupert Westenthaler | 15.06.2015 | Pre-review version |
| v0.2 | Milos Jovanovik, Carl Blakeley, Adrian Gschwend, Reto Gmür, Rupert Westenthaler | 22.06.2015 | Reviewed version |
| v0.3 | Milos Jovanovik, Carl Blakeley, Adrian Gschwend, Reto Gmür, Rupert Westenthaler | 29.06.2015 | Reviewed version |

#### Document Information ####

- Deliverable Nr Title: D5.4 Large-scale technical validation of Fusepool P3 platform
- Lead: Milos Jovanovik (OGL)
- Authors: Milos Jovanovik, Carl Blakeley (OGL), Adrian Gschwend (BUAS), Reto Gmür (BUAS), Rupert Westenthaler (SRFG)
- Publication Level: Public

#### Document Context Information ####

 - Project (Title/Number): Fusepool P3 (609696)
 - Work Package / Task: WP5 / T5.6
 - Responsible person and project partner: Milos Jovanovik (OGL)

#### Quality Assurance / Review ####

- 1st reviewer: Jakob Frank (SRFG)
- 2nd reviewer: Luigi Selmi (BUAS)

#### Official Citation ####
Fusepool-P3-D5.4

#### Copyright ####

This document contains material, which is the copyright of certain Fusepool P3 consortium parties. This work is licensed under the Creative Commons Attribution 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.

#### Acronyms and Abbreviations ####

| Acronym | Description |
| --- | :--- |
| DoW | Description of Work |
| DDC | Dewey Decimal Classification |
| ECC | Error correcting code |
| NER | Named entity recognition |
| NLP | Natural language processing |
| FP3 | Fusepool P3 |
| FAM | Fusepool Annotation Model |
| FST | Finite state transducer |
| GND | Gemeinsame Normdatei (Integrated Authority File) |
| LLC | Library of Congress Classification |
| POS | Part of speech |
| SSD | Solid-state drive |

## Executive Summary ##

This document details the work performed and planned as part of Deliverable D5.4 / Task 5.6 "Large-scale technical validation of the Fusepool P3 platform".

**Status, June 2015:** At the current time this is an **interim report** detailing our goals and approach to large scale technical validation of the FP3 Linked Data Platform. The validation work is currently on-going.  It is our intention to include the validation test results themselves in a revised version of this report once testing is complete. 

TO DO - Finish executive summary

...

The Library Keyword Clustering (LKC) use case centres on the interlinking of three large datasets:  two library catalogs, the [B3Kat](https://lod.b3kat.de/) catalog and the [GND catalog of the German National Library](http://www.dnb.de/EN/Standardisierung/GND/gnd.html),  and [DBpedia](http://dbpedia.org). 

The seed dataset, B3Kat, contains approximately 26 million bibliographic records. The three core datasets combined total around 2 billion triples. We estimate that the resulting data corpus after enrichment will be approaching 3 billion triples.

...

## Introduction ##

The DoW {TO DO: Link} for the Fusepool P3 project describes the main task of this deliverable as: 

*"T5.6 Technical validation of Fusepool Linked Data Platform: implementation will be tested through the large-scale real life use cases defined in WP1 to ensure that an industry-strength Linked Data platform is created...".*

In selecting use cases for large scale testing, our choice was influenced by two criteria: 

 - the test data should be sufficiently large to adequately test the ability of the FP3 platform to handle large data volumes;
 - processing of the test data should exercise as much of the platform as possible.

The use cases driving the development of the FP3 Platform have been defined as part of WP1 and are described in the Deliverable 1.1 report ["D1.1 Use cases and data specified, data modeled and prepared"](https://github.com/fusepoolP3/p3-wp1-deliverables/blob/master/d11-deliverable.md). From these, we selected one use case to form the basis of the large scale testing of the platform:

[Large Scale Validation Use Case: Library Keyword Clustering](https://github.com/fusepoolP3/p3-wp1-deliverables/blob/master/d11-deliverable.md#large-scale-validation-use-case-library-keyword-clustering)]

This use case focuses on exercising the bulk import and bulk transformation features of the platform, along with the ability to handle large datasets.

## Library Keyword Clustering (LKC) 

The "Linked Keyword Clustering" (LKC) use case is described in detail in the WP1 [Deliverable 1.1 report](https://github.com/fusepoolP3/p3-wp1-deliverables/blob/master/d11-deliverable.md#large-scale-validation-use-case-library-keyword-clustering)]. For convenience, we provide an overview below.

### Key Datasets
The LKC use case centres on the interlinking of three large RDF datasets:  two library catalogs - the [GND catalog of the German National Library](http://www.dnb.de/EN/Standardisierung/GND/gnd.html) and the [B3Kat](https://lod.b3kat.de/) catalog - and [DBpedia](http://dbpedia.org).

####GND Dataset

An [Integrated Authority File](http://en.wikipedia.org/wiki/Integrated_Authority_File) (German: Gemeinsame Normdatei) or GND is an international authority file for the organisation of personal names, subject headings and corporate bodies from catalogues. It is used mainly for documentation in libraries and increasingly also by archives and museums. GNDs is managed by the German National Library (Deutsche National Bibliothek) in cooperation with various library networks in German-speaking Europe and other partners.

The [Linked Data Service of the German National Library](http://www.dnb.de/EN/Service/DigitaleDienste/LinkedData/linkeddata_node.html) has made their entire [GND catalog available as Linked Data dumps](http://datendienst.dnb.de/cgi-bin/mabit.pl?userID=opendata&pass=opendata&cmd=login). The [GND RDF data dump](http://datendienst.dnb.de/cgi-bin/mabit.pl?userID=opendata&pass=opendata&cmd=login) is roughly 10GB, comprising approximately 10,900,000 records, translating to {TO DO} xxx triples.

{TO DO}: Add some more information about the Data contained in this Dataset

####B3Kat Dataset
[B3Kat](http://www.b3kat.de/) is the "common union catalogue" of [BVB](http://www.bib-bvb.de/) (Bavarian Library Network) and [KOBV](http://www.kobv.de/) (Cooperative Library Network Berlin-Brandenburg). The [B3Kat Linked Open Data Service](https://lod.b3kat.de/) provides an RDF catalog of 180 academic libraries in Bavaria, Berlin and Brandenburg, describing roughly 26 million titles. 

The [B3Kat RDF data dump](https://lod.b3kat.de/download) contains about 600-890 million RDF triples in 5.5GB-7.7GB of data.

{TO DO}: Add some more information about the Data contained in this Dataset

####DBpedia Dataset
The [DBpedia](http://dbpedia.org) dataset describes 4.58 million entities, including 1,445,000 persons, 735,000 places, and 241,000 organizations. The data set features labels and abstracts for these entities in up to 125 different languages; 25.2 million links to images and 29.8 million links to external web pages. It consists of 3 billion RDF triples and contains around 50 million links into other RDF datasets. [DBpedia dumps](http://wiki.dbpedia.org/Downloads) in 119 languages are available on the DBpedia download server.

{TO DO}: Are these numbers correct? 3 billion DBpedia alone? There is another number later in this document when we calculate the total amount of triples.


### Initial Dataset Connections

At the outset, before any interlinking by the FP3 framework, minimal connections existed between the source datasets. These minimal base connections, which provided the initial basis for richer interlinking and enrichment, are outlined below.

#### B3Kat - GND
The B3Kat dataset links to the GND corpus through GND IDs held as `dcterms:subject` values. 

The home page of the [B3Kat Linked Open Data Service](https://lod.b3kat.de/) includes a link for displaying an [example title](https://lod.b3kat.de/example). From this, it can be seen that the `dcterms:subject` property contains a number of subject identifiers, using several classification schemes include [GND](http://www.dnb.de/EN/Standardisierung/GND/gnd.html), [DDC](http://en.wikipedia.org/wiki/Dewey_Decimal_Classification) and [RVK](https://www.uni-erfurt.de/en/bibliothek/library-erfurt/research/classification-rvk/). The LKC use case makes reference to Hanns Eisler's "Deutsche Sinfonie" <https://lod.b3kat.de/page/title/BV001289953> as a central example - again the `dcterms:subject` property uses subject identifiers spanning the GND, DDC, [LLC](http://en.wikipedia.org/wiki/Library_of_Congress_Classification) and RVK schemes.

#### GND - DBpedia

The GND dataset includes links to DBpedia resource descriptions for many items, through `owl:sameAs` properties. For example, the GND entry for Hans Eisler includes:

    <rdf:Description rdf:about="http://d-nb.info/gnd/118529692">
        <rdf:type 
        rdf:resource="http://d-nb.info/standards/elementset/gnd#DifferentiatedPerson" />
        <foaf:page       rdf:resource="http://de.wikipedia.org/wiki/Hanns_Eisler" />
        <owl:sameAs rdf:resource="http://dbpedia.org/resource/Hanns_Eisler" />
        ...
        <gndo:surname>Eisler</gndo:surname>
        ...
    </rdf:Description>

###Primary Use Case Goals

Beyond providing a basis for large scale testing of the FP3 platform, the main goals from the user's perspective of this application of the platform are: 

 - make GNDs more usable;
 - aggregate information about concepts identified by GNDs and enrich them with NER and dictionary matching;
 - add context around a GND concept, identifying related concepts;
 - for each document in the B3Kat catalogue,  use the GND IDs associated with the document to identify the main subject concepts of the document;
 - identify related documents connected to the same subject(s);
 -  create a tag cloud of co-occurrent entities (places, persons, organizations ...) for each subject concept;
   - e.g. Which places are mentioned in the title text of books about Hanns Eisler? How often do they occur?

### Broad Approach

- Load the GND, B3Cat and DBpedia datasets into quad storage, making them available for querying via SPARQL.
- Use CONSTRUCT queries to create initial graphs, one graph per GND.
- From these graphs, generate approximately 25 million files containing mash-ups of the source RDF data, which will then be posted to a transforming LDP container.
- Once in LDP, the linked transformer triggers entity extraction and NER on book titles.
 
{TO DO}: To Technical for an Overview (e.g. use CONSTRUCT queries) ... focus on the description of the necessary steps. e.g. Import the RDF, Preprocess the RDF according to the use-case requirements. Enrich the RDF using the Fusepool Plattform. Host the enriched Data using the Fusepool Plattform (SPARQL and LDP).

## Test Infrastructure

### Estimated Dataset Sizes

The source datasets total approximately 2 billion triples:

| Source dataset | Approximate size |
| :-- | :-- |
| GND | 124 million triples |
| B3Cat | 900 million triples |
| Dbpedia | 1 billion triples (assuming the full dataset and associated ontologies are loaded) |

{TO DO}: These numbers do not correspond to the numbers above in the overview. Either we need to say that we use a subset of them or we correct it accordingly.

We estimate the resultant transformed aggregated data will comprise under 1 billion triples - a Turtle mockup of the desired output by the end users suggests that there will be ~200 triples per subject. Assuming the source data covers approximately 3 million authors, the output data will be of the order of 600 million triples held in LDP storage.

The estimated total data corpus size of 3 billion triples, together with past experience of the core software components to be exercised, was then used as the basis for specifying the test platform hardware.

###Software

#### FP3 Platform Deployment

For ease of deployment, the FP3 test platform will be composed from Docker images.

 -   [FP3 Platform reference implementation - Docker image details]( https://github.com/fusepoolP3/p3-platform-reference-implementation)
 - [VOS - Docker image details](https://github.com/fusepoolP3/virtuoso-docker)
 
#### Storage Layer

Although Marmotta has provided the RDF storage for much of the FP3 development, Virtuoso Open Source Edition (VOS) was chosen as the storage layer, to provide the quad store, LDP server and SPARQL endpoints. A number of evaluations have tested Virtuoso and found it to be scalable to the region of 1 billion+ triples. Virtuoso Commercial Edition was not considered necessary for the calculated use case dataset sizes. VOS supports multiple cores, so a single server instance running on a single machine is suitable. The GND, B3Cat and DBpedia datasets will be loaded directly into the Virtuoso quad store.

{TO DO}: 1 billion+ sounds not right? In terms of this should be higher or not?
 
### Hardware

The test platform comprises a dedicated server, not a virtual machine, with the following specification:

 - 2 x Intel Xeon E5 2620V2, 2 x (6 x 2.10 GHz)
 - 128 GB buffered ECC RAM
 - 1000 GB SSD (Samsung 840 EVO)
 - Ubuntu 15.04

####Memory Requirements

Based on the anticipated size of the datasets and generated data, we estimated the amount of RAM required to be roughly as follows:

 - Virtuoso 7 typically requires 10GB per billion triples (10 bytes per quad), depending on how well the particular datasets compress for storage. 32GB RAM for Virtuoso was considered sufficient for ~3 billion triples. In general, memory is more important than disk storage, as once the database is 'warm', i.e. the database working set is in memory, then disk access is minimized.
 - Stanbol's memory requirements were estimated to be ~4GB. For creating Solr indexes with the Entityhub Indexing Tool, Jena TDB will be used as the source for indexing. Jena TDB is not required at runtime. The entity labels are held in memory, encoded in a FST (~300MB for all DBpedia's English labels).
 - Other Fusepool Transformers memory requirements are limited to in-memory caching of currently processed items. Threr memory requirement will be far below 1GB.

##Data Transformation
### Source Dataset Loading

A prerequisite for transforming all this data was that the source datasets be loaded into the test LDP server. Virtuoso incorporates a bulk loader for importing large RDF datasets.  Although B3Kat and DBpedia provide public SPARQL endpoints (the German National Library does not make a SPARQL endpoint for GND catalog), it was not thought feasible (or responsible) to rely on these endpoints for such large scale processing.

{TO DO}: Outline the number of expected reuqests.

### Use Case Task Details

The use case description provided by the WP1 Deliverable 1.1 report lists the main processing steps necessary to provide the desired enrichment and interlinking. These are re-iterated again below, but with a focus on the technical details of realizing the data transformations.

Some of the use case tasks, primarily preparation of the input data in tasks 1 - 5, are to be performed by the application stakeholders (primarily Johannes Hercher, Free University Berlin). Tasks 6 - 8 will be performed by the FP3 platform itself and constitute the core of the validation testing.
 
####1: Make a graph per GND subject for each B3Kat record

Approximately half of B3Kat's 26 million bibliographic records about books include links to between 1 and 10 related GND subject identifiers. 

See for example Hanns Eisler’s Deutsche Sinfonie https://lod.b3kat.de/page/title/BV001289953. Using the [B3Kat SPARQL endpoint](https://lod.b3kat.de/doc/de/sparql-endpoint/):

     SELECT * WHERE { 
       <http://lod.b3kat.de/title/BV001289953> 
         dc:title ?title ;
         dct:subject ?subject 
         filter regex(str(?subject), "d-nb.info/gnd")
     }
    
returns
	
| title | subject |
| :-- | :-- |
| Hanns Eislers "Deutsche Sinfonie" | http://d-nb.info/gnd/118529692 |
| Hanns Eislers "Deutsche Sinfonie" | http://d-nb.info/gnd/4222905-4  |

For each GND ID listed as a `dct:subject` in a B3Kat record, a graph will be created by a CONSTRUCT query, using the GND ID as the graph URI.

####2: Get basic information about each GND subject

B3Kat contains only GND IDs and no additional information from the GND authority file is available. However, this information is available in the GND RDF dumps. For instance, alternative labels, geo information, semantic relations (broader, narrower, related), subject categories, mappings to Wikipedia and DBpedia. (See also Appendix A for an example of a typical entry from the GND dataset.)

Each graph created in step 1 will be augmented with information about the subject concept extracted from the subject description in the GND dataset.

####3: Get additional GND subject data from DBpedia

Many GND concepts have a link to DBpedia.  See Appendix A for an example;  the description of Eisler includes:

    <owl:sameAs rdf:resource="http://dbpedia.org/resource/Hanns_Eisler" /> 

This link will be used to fetch additional data, such as: `foaf:depiction`, `dbpedia-owl:thumbnail`, `dc:description`, `rdfs:comment`, `dbpedia-owl:abstract`, `dcterms:subject`, etc., to augment each base graph of step 1.

####4: Get co-occurrent concepts from B3Kat

For a given B3Kat concept, we can get details of other B3Kat concepts which are co-occurrent with it. For example, using the [B3Kat SPARQL endpoint](https://lod.b3kat.de/doc/de/sparql-endpoint/), we can find documents which reference concept Hanns Eisler in some way:

    SELECT distinct ?s ?p WHERE { 
     ?s ?p <http://d-nb.info/gnd/118529692>; # - Hanns Eisler
    } ORDER BY ?s LIMIT 500
returns:

| s | p |
| :-- | :-- |
| <http://lod.b3kat.de/title/BV000153283> | <http://id.loc.gov/vocabulary/relators/aut> |
| <http://lod.b3kat.de/title/BV000153283> |<http://purl.org/dc/terms/creator> |
| <http://lod.b3kat.de/title/BV000154417> | <http://purl.org/dc/terms/subject> |
| <http://lod.b3kat.de/title/BV000192526> | <http://id.loc.gov/vocabulary/relators/aut> |

The `dct:subject`s of the returned B3Kat documents provide co-occurrent GND concepts.

####5: Get texts from other B3Kat documents referencing the concept

For a given GND concept, we search for other documents which reference the concept in some way and extract their title and subtitle as the basis for NER.

e.g. Find the title and subtitle of documents which reference Hanns Eisler:

    PREFIX isbd: <http://iflastandards.info/ns/isbd/elements>

    SELECT DISTINCT ?s ?title ?subtitle
     WHERE { 
      ?s ?p <http://d-nb.info/gnd/118529692> ; # - Hanns Eisler 
     dc:title ?title ;
     optional { ?s isbd:P1006 ?subtitle }
    } LIMIT 500
    
####6: Perform NER on selected document texts

At this point, the graphs created and populated by steps 1 - 5 provide the base context for enrichment and interlinking by the FP3 platform. By some ETL process external to the platform and orchestrated by the end-user, each graph (1 per GND concept) will be used to generate an RDF document (of somewhere between 100 - 1000 triples) which will be deposited in a common transforming LDPC **[ISSUE]**. The transforming LDPC will be linked to an FP3 pipeline transformer combining the required transformers.

For each RDF document, we use the contained titles of books related to the core concept, and the DBpedia abstract for the core concept, as the input for NER analysis.

**ISSUE:** The Transforming LDPC Proxy in its current form takes a non-RDF resource and transforms it to RDF. This use case requires the Transforming Proxy to take an RDF resource as input.

 - What proxy modifications are required?
 - How will the input RDF resource be distinguished from the output RDF resource?

#####NER for LKC - Implementation

Stanbol supports a number of NLP/NER frameworks including Stanford NLP and OpenNLP. For this use case, OpenNLP will be used as it provides a basic NER model that can detect Persons, Organizations and Locations in German language texts. While there is also a German model for Stanford NLP this is only available for an older software version that is not compatible with the Stanford/Stanbol integration. As their are also English books to be processed OpenNLP will be configured with both German and English language models.

In common with most of the other Stanbol enhancement engines, OpenNLP accepts plain text. The [Literal Extraction Transformer]( https://github.com/fusepoolP3/p3-literal-extraction-transformer) will provide the plain text input to OpenNLP, by combining the Literal Extraction and Stanbol transformers in a pipeline. The Literal Extraction Transformer can be configured to iterate over specific literal predicates in particular languages, concatenating the text for the same language and emitting plain text.

#####NER for LKC - Expectations

Public available NER models typically include support for Persons, Organizations and Placeses and are trained based on news text corpora. In-domain (meaning when applied to news texts other than the training set) those systems typically reach F1 measures above 90%. However when appied to other type of texts the performance can drop drastically.

Expected input texts for the LKC domain will be very different from news texts. Texts are essentially concatenated titles and sub titles. Meaning that the texts will not contain full sentences nor will sentences be connected to previouse sentences.

Because of that we expect available NER modles to perform very badly on the data of this use case. Nevertheless, in spite of reservations about the quality of expected NER results,  we believe the use case is fit for purpose for validating that the platform can handle large amounts of data.

To obtain better results, it would be necessary to train specific NER models based on a training set on the LKC texts. Experience on other datasets suggests that one can train a reasonable good model with about 3000 manually annotated entities per entity type. We consider training an LKC-specific model to be outside the scope of this validation task.

Extracted Named Entities will be represented in the results by using the [Entity Mention Annotation](https://github.com/fusepoolP3/overall-architecture/blob/master/wp3/fp-anno-model/fp-anno-model.md#entity-mention-annotation) as defined by the Fusepool Annotation Model (FAM). The The [Literal Extraction Transformer]( https://github.com/fusepoolP3/p3-literal-extraction-transformer) consumes those annotations and converts them to explicit triples to be added to the enriched LKC dataset.

####7: Match thesauri concepts to each GND

In this step, we will perform *entity linking* against each GND ID with the aim to detect mentions of GND IDs in the titles and sub-titles of the LKC data. Mentioned GND Ids will be represented in the results by using [Entity Annotation](https://github.com/fusepoolP3/overall-architecture/blob/master/wp3/fp-anno-model/fp-anno-model.md#entity-annotation) as defined by the Fusepool Annotation Model (FAM).

NOTE: this is distinced from the *Named Entity Recognition* in step 6. *NER* detects types of Entities in the text without the need of any controlled vocabulary. This means that NER can detect new/unknown entities. *Entity linking* works based on a controlled vocabulary - in that case the GND. It uses the names (preffered and alternate) as input and looks for mentions of those in the text. If it finds such a mention is links the according entity with that mention in the text.

The Fusepool platform provides several entity linking implementations. Options include:
 
 * Dictionary Matching Transformer,
 * Stanbol Entityhub Linking Engine
 * Stanbol FST Linking Engine 

For this usecase we will use the [FST Linking egine](http://stanbol.apache.org/docs/trunk/components/enhancer/engines/lucenefstlinking) of Apache Stanbol as this one is most efficient for linking against vocabularies with the size of the GND. The index required by the FST Linking Engine will be build in a pre-processing step by using the RDF indexing tool provided by Apache Stanbol.

As with step 6, the Literal Extraction Transformer will be used to collect and concatinate the input text from literals of processed Entities. The Literal Extraction Transformer will also be used to consume the extraction results represented by the Fusepool Annotation Model (FAM) and convert them to explicit triples to be added to the enriched LKC data.

In addition to GND IDs Entity Linking will be also provided for the following three SKOS based thesauri:

 * The [Thesaurus for the Social Sciences](http://www.gesis.org/en/services/research/thesauri-und-klassifikationen/social-science-thesaurus/) ([The Soz](http://www.semantic-web-journal.net/sites/default/files/swj279_2.pdf)).
 * The [IPTC](https://iptc.org/) media topics [thesaurus](https://iptc.org/standards/media-topics/), a 1100-term taxonomy with a focus on categorizing text. The Media Topics vocabulary can be viewed on the [IPTC Controlled Vocabulary server]( http://cv.iptc.org/newscodes/mediatopic).
 * The [STW Thesaurus for Economics](http://zbw.eu/stw/versions/latest/download/about.en.html).
 

####8: Matching against DBpedia

Some entities are not covered by the GND, or the subject terminologies. In order to find additional related concepts in DBpedia and related Wiki pages in Wikipedia, we propose trying to find them using alternative entity linking services. Two candidate services are [dataTXT](http://dandelion.eu/datatxt/) and [DBpedia Spotlight](https://github.com/dbpedia-spotlight/dbpedia-spotlight/wiki). Both are available as Stanbol enhancement engines, accessible through the FP3 Stanbol Enhancer Transformer.

dataTXT is a named entity extraction & linking service that, given a plain text, gives back a set of entities found in the text and links to corresponding entries in Wikipedia. It also optionally returns the entity type from DBpedia. 

dataTXT performs very well even on short texts, on which many other similar services do not. It is based on co-references of entities, it does not use any NLP feature. Even if the input texts are not complete sentences, we expect it to return valid results. For this reason it is our preferred service for this step.

DBpedia Spotlight provides a fallback option. However, we feel it is unlikely to produce better results than dataTXT. Should we opt to try this option, we will configure our own DBpedia Spotlight service.

**TO DO:** More technical details of this processing step required

[Q->Adrian] Re: the use case example for step 8: "Cantata on the death of a comrade for singing..."

- Where is this text from?
- What is the B3Kat URI of this entity?
- Does this entity not have a corresponding GND ID?

##Data Consumption

The large scale validation should include examples of consuming the enriched/interlinked data. For this project, we believe it is sufficient to provide example SPARQL queries illustrating how the Linked Data might be consumed, or at most create a simple user interface.

TO DO: Sample SPARQL queries.

##Validation Metrics

TO DO

### System/Disk

* I think we should monitor disk IO & memory usage during the run so we can see what we used. Need to see what's state of the art on Linux as of now

### Network/HTTP
* To properly analyze performance we need to know how our HTTP requests to transformer & pipeline are doing
* Capturing it in the proxy itself will not catch individual transformers in pipeline
* To do that we either have to log in each transformer itself and analyze it later or we grab what's going on on the network with pcap (tcpdump)
* There is a [spec](http://www.softwareishard.com/blog/har-12-spec/) from Google to analyze such data, it's called [HAR](https://www.igvita.com/2012/08/28/web-performance-power-tool-http-archive-har/), export for this is built in in Chrome for example
* There seems to be ways to generate that out of `pcap` files, for example [pcap2har](https://github.com/andrewf/pcap2har)
* There are tools with frontends, for example [harviewer](https://code.google.com/p/harviewer/) or [PCAP Web Performance Analyzer](https://pcapperf.appspot.com/)
* We would need to see if we get the information we need (probably mainly response time)
* We would have to script something on our own to analyze the results
* I'm not sure about the code, the repositories look pretty much unmaintained so no clue if the code still works (seems mainly Python)

## Validation Results

The validation results will be provided once available.

## Conclusions and Future Work ##

TO DO

Should describe that this is work in progress and this document will be updated within the next months.

## References ##

TO DO

## Appendix A - GND RDF Description of Hanns Eisler

Below is the description of Hanns Eisler extracted from the GND dataset, illustrating a typical GND RDF description.

    <rdf:Description rdf:about="http://d-nb.info/gnd/118529692">
    	<rdf:type rdf:resource="http://d-nb.info/standards/elementset/gnd#DifferentiatedPerson" />
    	<foaf:page rdf:resource="http://de.wikipedia.org/wiki/Hanns_Eisler" />
    	<owl:sameAs rdf:resource="http://dbpedia.org/resource/Hanns_Eisler" />
    	<owl:sameAs rdf:resource="http://viaf.org/viaf/19865132" />
    	<gndo:gndIdentifier>118529692</gndo:gndIdentifier>
    	<gndo:oldAuthorityNumber>(DE-588a)118529692</gndo:oldAuthorityNumber>
    	<gndo:oldAuthorityNumber>(DE-588a)185686249</gndo:oldAuthorityNumber>
    	<gndo:oldAuthorityNumber>(DE-588a)139523375</gndo:oldAuthorityNumber>
    	<gndo:oldAuthorityNumber>(DE-588a)134366263</gndo:oldAuthorityNumber>
    	<gndo:oldAuthorityNumber>(DE-101c)310056268</gndo:oldAuthorityNumber>
    	<gndo:oldAuthorityNumber>(DE-588c)4014114-7</gndo:oldAuthorityNumber>
    	<gndo:variantNameForThePerson>Eisler, ...</gndo:variantNameForThePerson>
    	<gndo:variantNameEntityForThePerson rdf:parseType="Resource">
    		<gndo:forename>...</gndo:forename>
    		<gndo:surname>Eisler</gndo:surname>
    	</gndo:variantNameEntityForThePerson>
    	<gndo:variantNameForThePerson>Eissler, Hanns</gndo:variantNameForThePerson>
    	<gndo:variantNameEntityForThePerson rdf:parseType="Resource">
    		<gndo:forename>Hanns</gndo:forename>
    		<gndo:surname>Eissler</gndo:surname>
    	</gndo:variantNameEntityForThePerson>
    	<gndo:variantNameForThePerson>Eisler, Hans</gndo:variantNameForThePerson>
    	<gndo:variantNameEntityForThePerson rdf:parseType="Resource">
    		<gndo:forename>Hans</gndo:forename>
    		<gndo:surname>Eisler</gndo:surname>
    	</gndo:variantNameEntityForThePerson>
    	<gndo:variantNameForThePerson>Eisler, Johannes</gndo:variantNameForThePerson>
    	<gndo:variantNameEntityForThePerson rdf:parseType="Resource">
    		<gndo:forename>Johannes</gndo:forename>
    		<gndo:surname>Eisler</gndo:surname>
    	</gndo:variantNameEntityForThePerson>
    	<gndo:preferredNameForThePerson>Eisler, Hanns</gndo:preferredNameForThePerson>
    	<gndo:preferredNameEntityForThePerson rdf:parseType="Resource">
    		<gndo:forename>Hanns</gndo:forename>
    		<gndo:surname>Eisler</gndo:surname>
    	</gndo:preferredNameEntityForThePerson>
    	<gndo:familialRelationship rdf:resource="http://d-nb.info/gnd/116435410" />
    	<gndo:familialRelationship rdf:resource="http://d-nb.info/gnd/124362214" />
    	<gndo:familialRelationship rdf:resource="http://d-nb.info/gnd/118691392" />
    	<gndo:familialRelationship rdf:resource="http://d-nb.info/gnd/118681850" />
    	<gndo:familialRelationship rdf:resource="http://d-nb.info/gnd/1054173877" />
    	<gndo:professionOrOccupation rdf:resource="http://d-nb.info/gnd/4032009-1" />
    	<gndo:professionOrOccupation rdf:resource="http://d-nb.info/gnd/4040841-3" />
    	<gndo:playedInstrument rdf:resource="http://d-nb.info/gnd/4030982-4" />
    	<gndo:playedInstrument rdf:resource="http://d-nb.info/gnd/4057587-1" />
    	<gndo:gndSubjectCategory rdf:resource="http://d-nb.info/standards/vocab/gnd/gnd-sc#14.4p" />
    	<gndo:geographicAreaCode rdf:resource="http://d-nb.info/standards/vocab/gnd/geographic-area-code#XA-DE" />
    	<gndo:geographicAreaCode rdf:resource="http://d-nb.info/standards/vocab/gnd/geographic-area-code#XD-US" />
    	<gndo:geographicAreaCode rdf:resource="http://d-nb.info/standards/vocab/gnd/geographic-area-code#XA-AT" />
    	<gndo:placeOfBirth rdf:resource="http://d-nb.info/gnd/4035206-7" />
    	<gndo:placeOfDeath rdf:resource="http://d-nb.info/gnd/4005728-8" />
    	<gndo:placeOfActivity rdf:resource="http://d-nb.info/gnd/4042011-5" />
    	<gndo:placeOfExile rdf:resource="http://d-nb.info/gnd/4078704-7" />
    	<owl:sameAs rdf:resource="http://www.filmportal.de/person/18AC4FE0900B4565A8D821ED5F6A175E" />
    	<gndo:gender rdf:resource="http://d-nb.info/standards/vocab/gnd/Gender#male" />
    	<gndo:dateOfBirth rdf:datatype="http://www.w3.org/2001/XMLSchema#date">1898-07-06</gndo:dateOfBirth>
    	<gndo:dateOfDeath rdf:datatype="http://www.w3.org/2001/XMLSchema#date">1962-09-06</gndo:dateOfDeath>
    </rdf:Description>

## Appendix B - Overview Notes on Apache Stanbol

This appendix provides a brief overview of the main components of Apache Stanbol relevant to the LKC use case, with the aim of supplying some background context to the technical points raised in the main document, for readers unfamiliar with Stanbol.

### Information Extraction for the LKC use case

For the LKC use case the following information extraction workflow is expected

![Information Extraction Workflow](d5.4-info-extraction-workflow.png)

The [Literal Extraction Transformer](https://github.com/fusepoolP3/p3-literal-extraction-transformer) is used to collect literals of the source RDF data to be used for information extraction. It also processes the information extraction results encoded using the [Fusepool Annotation Model](https://github.com/fusepoolP3/overall-architecture/blob/master/wp3/fp-anno-model/fp-anno-model.md) (FAM) and converts them to simple statements for the enriched RDF data.

The Literal Extraction Transformer is configured with a second transformer doing the actual information extraction work. As for the LKC use case Apache Stanbol is used for the information extraction work the [Stanbol Enhancer Transformer](https://github.com/fusepoolP3/p3-stanbol-enhancer-adapter/tree/master/service) is configured. This "transformer adapter" for the Apache Stanbol Enhancer allows to use Stanbol Enhancement Chains or single Enhancement Engines as Fusepool Transformers.

As the LKC use case requires several information extraction capabilities a Enhancement Chain supporting (1) Named Entity Linking , (2) Entity Linking against the GND, GESIS, STW and IPTC and (3) DBpedia linking by using the DataTXT engine is configured. As Stanbol uses the FISE annotation model also an engine that converts FISE into the Fusepool Annotation Model is required.

All the transformers and Stanbol components mentioned above are described in detail in D3.1: Section 9 provides details about the Fusepool Annotation Model; Section 10.1 gives details about the integration of Apache Stanbol to the Fusepool Plattform as Transformer. Section 10.2 provides details about the DataTXT integration.

The remainder of this appendix aims to provide a short overview on Apache Stanbol components used for the LKC use case.

### Enhancement Chain

An [Enhancement Chain](http://stanbol.apache.org/docs/trunk/components/enhancer/chains/) defines how content parsed to the Stanbol Enhancer is processed. More concretely it defines which [Enhancement Engines](http://stanbol.apache.org/docs/trunk/components/enhancer/engines) and in what order are used to process the parsed content. For the LKC use case an Enhancement chain with all the required Information Extraction Capabilities needs to be configured.

A typical Stanbol enhancement workflow is depicted below:

![Typical Stanbol enhancement workflow](https://stanbol.apache.org/docs/trunk/enhancementworkflow.png)

The typical information extraction workflow used with Apache Stanbol starts with a pre-processing step. In this step the parsed content is prepared for later information extraction. Plain Text extraction from rich text documents is a typical task performed in this step. For the LKC use case this step is not required as this work is already done by the [Literal Extraction Transformer](https://github.com/fusepoolP3/p3-literal-extraction-transformer) outside of Apache Stanbol.

The second step is about natural language processing. For the LKC this includes Language Detection and Named Entity Recognition. As OpenNLP requires Sentence Detection and Tokenization before NER those need also to be configured.

In the Semantic Lifting step the Entity Linking against the four controlled vocabularies and the dataTXT linking to DBPedia will take place.

In the post processing phase the FISE enhancements will be refactored to the Fusepool Annotation Model (FAM).

Apache Stanbol provides different Enhancement Chain implementations. Typically users want to use the [Weighted Chain](http://stanbol.apache.org/docs/trunk/components/enhancer/chains/weightedchain) as this supports automatic ordering of configured enhancement engines. So for the user it is sufficient to provide the list of required engines. The ordering is not of importance.

### Enhancement Engines

An Enhancement Engine is an information extraction component. It can extract plain text from a PDF file; detect the language of a text of a chapter; extract Named Entities like Persons or Placed, detect Mentions of Entities managed in a Controlled vocabulary; disambiguate results of previous engines or just refactor annotations to a different annotation model. 

Apache Stanbol comes with a wide range of Enhancement Engines and users can also implement and use customs one. For the LKC use case the following engines are required:

* Language Detection: The most common Language detection engine for Apache Stanbol is the [Langdetect Engine](http://stanbol.apache.org/docs/trunk/components/enhancer/engines/langdetectengine)
* Named Entity Recognition: For the use case [Apache OpenNLP](http://opennlp.apache.org/) will be used. For Named Entity Recognition the OpenNLP [Sentence Detection](http://stanbol.apache.org/docs/trunk/components/enhancer/engines/opennlpsentence), [Tokenizer](http://stanbol.apache.org/docs/trunk/components/enhancer/engines/opennlptokenizer) and [NER](http://stanbol.apache.org/docs/trunk/components/enhancer/engines/opennlpner) engines will get used.
* Entity Linking: As the texts for the LKC are not full sentences a linking engine that can link against all words in the text is best suited (PLAIN linking mode). For such a scenario the [FST Linking Engine](http://stanbol.apache.org/docs/trunk/components/enhancer/engines/lucenefstlinking) is best suited. It is based on The Apache Lucene FST (Finit State Transducer) API witch allows it to hold labels of large vocabularies fully in memory (e.g. for all English labels of DBPedia one needs less as 300MByte of memory). 
* DBPedia Linking: While the FST linking engine could also be used for DBPedia it does not provide support for disambiguation. Because of the we will use the [DataTXT engine](https://github.com/fusepoolP3/p3-datatxt-stanbol) instead. This engine is based on the [DataTXT](https://dandelion.eu/products/datatxt/) service.
* Fusepool Annotation Model support: Apache Stanbol uses the [Stanbol Enhancement Structure](http://stanbol.apache.org/docs/trunk/components/enhancer/enhancementstructure). When used as Fusepool Transformer those annotations need to be converted to the Fusepool Annotation Model. The [FAM engine](https://github.com/fusepoolP3/p3-stanbol-engine-fam) implements this conversion.

### Working with Custom Vocabularies

For using Entity Linking Engines it is required to create special indexes over custom vocabularies. For small and medium sized vocabularies those can be created while uploading them to Apache Stanbol. Hoever for big vocabularies or if one want to have more control on how those indexes are build Apache Stanbol provides a special batch processing tool for creating those.

The whole process is described by the [Working with Custom Vocabularies](https://stanbol.apache.org/docs/trunk/customvocabulary.html) usage scenario.

Here are the main Steps of the process:

* Build the Entityhub Indexing Tool and grab the binary `org.apache.stanbol.entityhub.indexing.genericrdf-*-jar-with-dependencies.jar`
* call the binary with the `init` command to initialise the configuration hierarchy
* edit the `indexing/conig/indexing.properties` file. See comments in this file for more information
* copy the RDF files of your controlled vocabulary to the `indexing/resources/rdfdata` folder
* call the binary with the `index` command to index the vocabulary
* after the indexing finishes the `indexing/dist` folder will contain two files
    1. `org.apache.stanbol.data.site.{name}-{version}.jar` an OSGI bundle to be installed to the Stanbol runtime. This Bundle provides the Entityhub configuration for the indexed dataset
    2. `{name}.solrindex.zip` an archived Solr core. This needs to be installed as data file to apache stanbol. The easiest way to do this is to copy it to the `stanbol/datafiles` folder of your stanbol instance.
    
After performing those steps you will have an Entityhub Site holding your Custom Vocabulary ready to be used with any of the Entity Linking Engines provided by Apache Stanbol. 

### Stanbol EntityHub

The [Entityhub](https://stanbol.apache.org/docs/trunk/components/entityhub) is the Stanbol component responsible for providing the information about entities relevant to the users domain.  For the LKC use case it will be used to manage the indexes of the Vocabulary used for Entity Linking. Those vocabularies will include

* The [[GND catalog of the German National Library](http://www.dnb.de/EN/Standardisierung/GND/gnd.html)
* The [Thesaurus for the Social Sciences](http://www.gesis.org/en/services/research/thesauri-und-klassifikationen/social-science-thesaurus/) ([The Soz](http://www.semantic-web-journal.net/sites/default/files/swj279_2.pdf)).
* The [IPTC](https://iptc.org/) media topics [thesaurus](https://iptc.org/standards/media-topics/), a 1100-term taxonomy with a focus on categorizing text. The Media Topics vocabulary can be viewed on the [IPTC Controlled Vocabulary server]( http://cv.iptc.org/newscodes/mediatopic).
* The [STW Thesaurus for Economics](http://zbw.eu/stw/versions/latest/download/about.en.html).

All those indexes will be build using the Entityhub Indexing Tool and afterwards be installed to the Stanbol Runtime.
