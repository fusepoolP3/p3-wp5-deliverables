#Large-scale Technical Validation of Fusepool P3 Platform 
###Deliverable D5.4

![enter image description here](https://avatars0.githubusercontent.com/u/5859504?v=3&s=200)

[TOC]

----------

#### Document History ####

| Ver. | Name | Date | Remark |
| :---: | :--- | :---: | :--- |
| v0.0 | Carl Blakeley, Milos Jovanovik | 08.06.2015 | Initial draft |
| v0.1 | Milos Jovanovik, Carl Blakeley, Adrian Gschwend, Reto Gmur, Rupert Westenthaler | 15.06.2015 | Pre-review version |
| v0.2 | Milos Jovanovik, Carl Blakeley, Adrian Gschwend, Reto Gmur, Rupert Westenthaler | 22.06.2015 | Reviewed version |
| v0.3 | Milos Jovanovik, Carl Blakeley, Adrian Gschwend, Reto Gmur, Rupert Westenthaler | 29.06.2015 | Reviewed version |

#### Document Information ####

- Deliverable Nr Title: D5.4 Large-scale technical validation of Fusepool P3 platform
- Lead: Milos Jovanovik (OGL)
- Authors: Milos Jovanovik, Carl Blakeley (OGL), Adrian Gschwend (BUAS), Reto Gmur (BUAS), Rupert Westenthaler (SRFG)
- Publication Level: Public

#### Document Context Information ####

 - Project (Title/Number): Fusepool P3 (609696)
 - Work Package / Task: WP5 / T5.6
 - Responsible person and project partner: Milos Jovanovik (OGL)

#### Quality Assurance / Review ####

- 1st reviewer: Jakob Frank (SRFG)
- 2nd reviewer: Luigi Selmi (BUAS)

#### Official Citation ####
Fusepool-P3-D5.4

#### Copyright ####

This document contains material, which is the copyright of certain Fusepool P3 consortium parties. This work is licensed under the Creative Commons Attribution 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.

#### Acronyms and Abbreviations ####

| Acronym | Description |
| --- | :--- |
| DoW | Description of Work |
| DDC | Dewey Decimal Classification |
| ECC | Error correcting code |
| NER | Named entity recognition |
| NLP | Natural language processing |
| FP3 | Fusepool P3 |
| FAM | Fusepool Annotation Model |
| FST | Finite state transducer |
| GND | Gemeinsame Normdatei (Integrated Authority File) |
| LLC | Library of Congress Classification |
| POS | Part of speech |
| SSD | Solid-state drive |

## Executive Summary ##

This document details the work performed and planned as part of Deliverable D5.4 / Task 5.6 "Large-scale technical validation of the Fusepool P3 platform".

**Status, June 2015:** At the current time this is an **interim report** detailing our goals and approach to large scale technical validation of the FP3 Linked Data Platform. The validation work is currently on-going.  It is our intention to include the validation test results themselves in a revised version of this report once testing is complete. 

TO DO - Finish executive summary

...

The Library Keyword Clustering (LKC) use case centres on the interlinking of three large datasets:  two library catalogs, the [B3Kat](https://lod.b3kat.de/) catalog and the [GND catalog of the German National Library](http://www.dnb.de/EN/Standardisierung/GND/gnd.html),  and [DBpedia](http://dbpedia.org). 

The seed dataset, B3Kat, contains approximately 26 million bibliographic records. The three core datasets combined total around 2 billion triples. We estimate that the resulting data corpus after enrichment will be approaching 3 billion triples.

...

## Introduction ##

The DoW {TO DO: Link} for the Fusepool P3 project describes the main task of this deliverable as: 

*"T5.6 Technical validation of Fusepool Linked Data Platform: implementation will be tested through the large-scale real life use cases defined in WP1 to ensure that an industry-strength Linked Data platform is created...".*

In selecting use cases for large scale testing, our choice was influenced by two criteria: 

 - the test data should be sufficiently large to adequately test the ability of the FP3 platform to handle large data volumes;
 - processing of the test data should exercise as much of the platform as possible.

The use cases driving the development of the FP3 Platform have been defined as part of WP1 and are described in the Deliverable 1.1 report ["D1.1 Use cases and data specified, data modeled and prepared"](https://github.com/fusepoolP3/p3-wp1-deliverables/blob/master/d11-deliverable.md) {TO DO - Check / update link}. From these, we selected one use case to form the basis of the large scale testing of the platform:

- Library Keyword Clustering [[Rough draft](http://ktk.netlabs.org/misc/2015-04-24_keywordlinking_usecase_fp3/readme.html)] {TO DO - Remove link to draft} 

This use case focuses on exercising the bulk import and bulk transformation features of the platform, along with the ability to handle large datasets.

## Library Keyword Clustering (LKC) 

The "Linked Keyword Clustering" (LKC) use case is described in detail in the WP1 [Deliverable 1.1 report](https://github.com/fusepoolP3/p3-wp1-deliverables/blob/master/d11-deliverable.md). {TO DO - Check / update link} [[Rough draft](http://ktk.netlabs.org/misc/2015-04-24_keywordlinking_usecase_fp3/readme.html)] For convenience, we provide an overview below.

### Key Datasets
The LKC use case centres on the interlinking of three large datasets:  two library catalogs - the [GND catalog of the German National Library](http://www.dnb.de/EN/Standardisierung/GND/gnd.html) and the [B3Kat](https://lod.b3kat.de/) catalog - and [DBpedia](http://dbpedia.org).

####GND Dataset

An [Integrated Authority File](http://en.wikipedia.org/wiki/Integrated_Authority_File) (German: Gemeinsame Normdatei) or GND is an international authority file for the organisation of personal names, subject headings and corporate bodies from catalogues. It is used mainly for documentation in libraries and increasingly also by archives and museums. GNDs is managed by the German National Library (Deutsche National Bibliothek) in cooperation with various library networks in German-speaking Europe and other partners.

The [Linked Data Service of the German National Library](http://www.dnb.de/EN/Service/DigitaleDienste/LinkedData/linkeddata_node.html) has made their entire [GND catalog available as Linked Data dumps](http://datendienst.dnb.de/cgi-bin/mabit.pl?userID=opendata&pass=opendata&cmd=login). The [GND RDF data dump](http://datendienst.dnb.de/cgi-bin/mabit.pl?userID=opendata&pass=opendata&cmd=login) is roughly 10GB, comprising approximately 10,900,000 records, translating to {TO DO} xxx triples.

####B3Kat Dataset
[B3Kat](http://www.b3kat.de/) is the "common union catalogue" of [BVB](http://www.bib-bvb.de/) (Bavarian Library Network) and [KOBV](http://www.kobv.de/) (Cooperative Library Network Berlin-Brandenburg). The [B3Kat Linked Open Data Service](https://lod.b3kat.de/) provides an RDF catalog of 180 academic libraries in Bavaria, Berlin and Brandenburg, describing roughly 26 million titles. 

The [B3Kat RDF data dump](https://lod.b3kat.de/download) contains about 600-890 million RDF triples in 5.5GB-7.7GB of data.

####DBpedia Dataset
The [DBpedia](http://dbpedia.org) dataset describes 4.58 million entities, including 1,445,000 persons, 735,000 places, and 241,000 organizations. The data set features labels and abstracts for these entities in up to 125 different languages; 25.2 million links to images and 29.8 million links to external web pages. It consists of 3 billion RDF triples and contains around 50 million links into other RDF datasets. [DBpedia dumps](http://wiki.dbpedia.org/Downloads) in 119 languages are available on the DBpedia download server.

### Initial Dataset Connections

At the outset, before any interlinking by the FP3 framework, minimal connections existed between the source datasets. These minimal base connections, which provided the initial basis for richer interlinking and enrichment, are outlined below.

#### B3Kat - GND
The B3Kat dataset links to the GND corpus through GND IDs held as `dcterms:subject` values. 

The home page of the [B3Kat Linked Open Data Service](https://lod.b3kat.de/) includes a link for displaying an [example title](https://lod.b3kat.de/example). From this, it can be seen that the `dcterms:subject` property contains a number of subject identifiers, using several classification schemes include [GND](http://www.dnb.de/EN/Standardisierung/GND/gnd.html), [DDC](http://en.wikipedia.org/wiki/Dewey_Decimal_Classification) and [RVK](https://www.uni-erfurt.de/en/bibliothek/library-erfurt/research/classification-rvk/). The LKC use case makes reference to Hanns Eisler's "Deutsche Sinfonie" <https://lod.b3kat.de/page/title/BV001289953> as a central example - again the `dcterms:subject` property uses subject identifiers spanning the GND, DDC, [LLC](http://en.wikipedia.org/wiki/Library_of_Congress_Classification) and RVK schemes.

#### GND - DBpedia

The GND dataset includes links to DBpedia resource descriptions for many items, through `owl:sameAs` properties. For example, the GND entry for Hans Eisler includes:

    <rdf:Description rdf:about="http://d-nb.info/gnd/118529692">
        <rdf:type 
        rdf:resource="http://d-nb.info/standards/elementset/gnd#DifferentiatedPerson" />
        <foaf:page       rdf:resource="http://de.wikipedia.org/wiki/Hanns_Eisler" />
        <owl:sameAs rdf:resource="http://dbpedia.org/resource/Hanns_Eisler" />
        ...
        <gndo:surname>Eisler</gndo:surname>
        ...
    </rdf:Description>

###Primary Use Case Goals

Beyond providing a basis for large scale testing of the FP3 platform, the main goals from the user's perspective of this application of the platform are: 

 - make GNDs more usable;
 - aggregate information about concepts identified by GNDs and enrich them with NER and dictionary matching;
 - add context around a GND concept, identifying related concepts;
 - for each document in the B3Kat catalogue,  use the GND IDs associated with the document to identify the main subject concepts of the document;
 - identify related documents connected to the same subject(s);
 -  create a tag cloud of co-occurrent entities (places, persons, organizations ...) for each subject concept;
   - e.g. Which places are mentioned in the title text of books about Hanns Eisler? How often do they occur?

### Broad Approach

- Load the GND, B3Cat and DBpedia datasets into quad storage, making them available for querying via SPARQL.
- Use CONSTRUCT queries to create initial graphs, one graph per GND.
- From these graphs, generate approximately 25 million files containing mash-ups of the source RDF data, which will then be posted to a transforming LDP container.
- Once in LDP, the linked transformer triggers entity extraction and NER on book titles.

## Test Infrastructure

### Estimated Dataset Sizes

The source datasets total approximately 2 billion triples:

| Source dataset | Approximate size |
| :-- | :-- |
| GND | 124 million triples |
| B3Cat | 900 million triples |
| Dbpedia | 1 billion triples (assuming the full dataset and associated ontologies are loaded) |

We estimate the resultant transformed aggregated data will comprise under 1 billion triples - a Turtle mockup of the desired output by the end users suggests that there will be ~200 triples per subject. Assuming the source data covers approximately 3 million authors, the output data will be of the order of 600 million triples held in LDP storage.

The estimated total data corpus size of 3 billion triples, together with past experience of the core software components to be exercised, was then used as the basis for specifying the test platform hardware.

###Software

#### FP3 Platform Deployment

For ease of deployment, the FP3 test platform will be composed from Docker images.

 -   [FP3 Platform reference implementation - Docker image details]( https://github.com/fusepoolP3/p3-platform-reference-implementation)
 - [VOS - Docker image details](https://github.com/fusepoolP3/virtuoso-docker)
 
#### Storage Layer

Although Marmotta has provided the RDF storage for much of the FP3 development, Virtuoso Open Source Edition (VOS) was chosen as the storage layer, to provide the quad store, LDP server and SPARQL endpoints. A number of evaluations have tested Virtuoso and found it to be scalable to the region of 1 billion+ triples. Virtuoso Commercial Edition was not considered necessary for the calculated use case dataset sizes. VOS supports multiple cores, so a single server instance running on a single machine is suitable. The GND, B3Cat and DBpedia datasets will be loaded directly into the Virtuoso quad store.
 
### Hardware

The test platform comprises a dedicated server, not a virtual machine, with the following specification:

 - 2 x Intel Xeon E5 2620V2, 2 x (6 x 2.10 GHz)
 - 128 GB buffered ECC RAM
 - 1000 GB SSD (Samsung 840 EVO)
 - Ubuntu 15.04

####Memory Requirements

Based on the anticipated size of the datasets and generated data, we estimated the amount of RAM required to be roughly as follows:

 - Virtuoso 7 typically requires 10GB per billion triples (10 bytes per quad), depending on how well the particular datasets compress for storage. 32GB RAM for Virtuoso was considered sufficient for ~3 billion triples. In general, memory is more important than disk storage, as once the database is 'warm', i.e. the database working set is in memory, then disk access is minimized.
 - Stanbol's memory requirements were estimated to be ~4GB. For creating Solr indexes with the Entityhub Indexing Tool, Jena TDB will be used as the source for indexing. Jena TDB is not required at runtime. The entity labels are held in memory, encoded in a FST (~300MB for all DBpedia's English labels).

##Data Transformation
### Source Dataset Loading

A prerequisite for transforming all this data was that the source datasets be loaded into the test LDP server. Virtuoso incorporates a bulk loader for importing large RDF datasets.  Although B3Kat and DBpedia provide public SPARQL endpoints (the German National Library does not make a SPARQL endpoint for GND catalog), it was not thought feasible (or responsible) to rely on these endpoints for such large scale processing.

### Use Case Task Details

The use case description provided by the WP1 Deliverable 1.1 report lists the main processing steps necessary to provide the desired enrichment and interlinking. These are re-iterated again below, but with a focus on the technical details of realizing the data transformations.

Some of the use case tasks, primarily preparation of the input data in tasks 1 - 5, are to be performed by the application stakeholders (primarily Johannes Hercher, Free University Berlin). Tasks 6 - 8 will be performed by the FP3 platform itself and constitute the core of the validation testing.
 
####1: Make a graph per GND subject for each B3Kat record

Approximately half of B3Kat's 26 million bibliographic records about books include links to between 1 and 10 related GND subject identifiers. 

See for example Hanns Eisler’s Deutsche Sinfonie https://lod.b3kat.de/page/title/BV001289953. Using the [B3Kat SPARQL endpoint](https://lod.b3kat.de/doc/de/sparql-endpoint/):

     SELECT * WHERE { 
       <http://lod.b3kat.de/title/BV001289953> 
         dc:title ?title ;
         dct:subject ?subject 
         filter regex(str(?subject), "d-nb.info/gnd")
     }
    
returns
	
| title | subject |
| :-- | :-- |
| Hanns Eislers "Deutsche Sinfonie" | http://d-nb.info/gnd/118529692 |
| Hanns Eislers "Deutsche Sinfonie" | http://d-nb.info/gnd/4222905-4  |

For each GND ID listed as a `dct:subject` in a B3Kat record, a graph will be created by a CONSTRUCT query, using the GND ID as the graph URI.

####2: Get basic information about each GND subject

B3Kat contains only GND IDs and no additional information from the GND authority file is available. However, this information is available in the GND RDF dumps. For instance, alternative labels, geo information, semantic relations (broader, narrower, related), subject categories, mappings to Wikipedia and DBpedia. (See also Appendix A for an example of a typical entry from the GND dataset.)

Each graph created in step 1 will be augmented with information about the subject concept extracted from the subject description in the GND dataset.

####3: Get additional GND subject data from DBpedia

Many GND concepts have a link to DBpedia.  See Appendix A for an example;  the description of Eisler includes:

    <owl:sameAs rdf:resource="http://dbpedia.org/resource/Hanns_Eisler" /> 

This link will be used to fetch additional data, such as: `foaf:depiction`, `dbpedia-owl:thumbnail`, `dc:description`, `rdfs:comment`, `dbpedia-owl:abstract`, `dcterms:subject`, etc., to augment each base graph of step 1.

####4: Get co-occurrent concepts from B3Kat

For a given B3Kat concept, we can get details of other B3Kat concepts which are co-occurrent with it. For example, using the [B3Kat SPARQL endpoint](https://lod.b3kat.de/doc/de/sparql-endpoint/), we can find documents which reference concept Hanns Eisler in some way:

    SELECT distinct ?s ?p WHERE { 
     ?s ?p <http://d-nb.info/gnd/118529692>; # - Hanns Eisler
    } ORDER BY ?s LIMIT 500
returns:

| s | p |
| :-- | :-- |
|<http://lod.b3kat.de/title/BV000153283> | <http://id.loc.gov/vocabulary/relators/aut> |
| <http://lod.b3kat.de/title/BV000153283> |<http://purl.org/dc/terms/creator> |
| <http://lod.b3kat.de/title/BV000154417> | <http://purl.org/dc/terms/subject> |
| <http://lod.b3kat.de/title/BV000192526> | <http://id.loc.gov/vocabulary/relators/aut> |

The `dct:subject`s of the returned B3Kat documents provide co-occurrent GND concepts.

####5: Get texts from other B3Kat documents referencing the concept

For a given GND concept, we search for other documents which reference the concept in some way and extract their title and subtitle as the basis for NER.

e.g. Find the title and subtitle of documents which reference Hanns Eisler:

    PREFIX isbd: <http://iflastandards.info/ns/isbd/elements>

    SELECT DISTINCT ?s ?title ?subtitle
     WHERE { 
      ?s ?p <http://d-nb.info/gnd/118529692> ; # - Hanns Eisler 
     dc:title ?title ;
     optional { ?s isbd:P1006 ?subtitle }
    } LIMIT 500
    
####6: Perform NER on selected document texts

At this point, the graphs created and populated by steps 1 - 5 provide the base context for enrichment and interlinking by the FP3 platform. By some ETL process external to the platform and orchestrated by the end-user, each graph (1 per GND concept) will be used to generate an RDF document (of somewhere between 100 - 1000 triples) which will be deposited in a common transforming LDPC **[ISSUE]**. The transforming LDPC will be linked to an FP3 pipeline transformer combining the required transformers.

For each RDF document, we use the contained titles of books related to the core concept, and the DBpedia abstract for the core concept, as the input for NER analysis.

**ISSUE:** The Transforming LDPC Proxy in its current form takes a non-RDF resource and transforms it to RDF. This use case requires the Transforming Proxy to take an RDF resource as input.

 - What proxy modifications are required?
 - How will the input RDF resource be distinguished from the output RDF resource?

#####NER for LKC - Implementation

Stanbol supports a number of NLP/NER frameworks including Stanford NLP and OpenNLP. For this use case, OpenNLP will be used - it is the only Stanbol NER framework which currently supports German language. The available German model for Stanford NLP is for v1.x, but the current integration of Stanford NLP with Stanbol requires at least Stanford NLP 2.x. OpenNLP will be configured for both German and English models.

In common with most of the other Stanbol enhancement engines, OpenNLP accepts plain text. The [Literal Extraction Transformer]( https://github.com/fusepoolP3/p3-literal-extraction-transformer) will provide the plain text input to OpenNLP, by combining the Literal Extraction and Stanbol transformers in a pipeline. The Literal Extraction Transformer can be configured to iterate over specific literal predicates in particular languages, concatenating the text for the same language and emitting plain text.

#####NER for LKC - Expectations

One of the report authors, Rupert Westenthaler, has extensive expertise in information extraction. Based on his experience and the type of texts being analyzed, he expects the quality of the NER results to be quite poor. Typical NER models expect news text. However, the source texts being analyzed in this use case are essentially concatenated titles,  not natural language text. Nevertheless, in spite of reservations about the expected NER results,  we believe the use case is fit for purpose for validating that the platform can handle large amounts of data.

To obtain better results, it would be necessary to train an NER topic model specifically for the LKC texts. Experience suggests that to obtain reasonable results, 3000 examples are needed for each entity type to be recognized. We consider training an LKC-specific model to be outside the scope of this project.

####7: Match thesauri concepts to each GND

In this step, we will perform *entity linking* against each GND ID, identifying related concepts, with the results represented as [FAM annotations](https://github.com/fusepoolP3/overall-architecture/blob/master/wp3/fp-anno-model/fp-anno-model.md), by keyword matching against controlled vocabularies. This is distinct from the *named entity linking* in step 6 which uses NER. The keyword matching is based only on label similarities.

A concatenation of the titles of books related to the core concept, and the DBpedia abstract for the core concept will provide the content for keyword linking. As with step 6, the Literal Extraction Transformer will provide the concatenated plain text for subsequent keyword matching.

The controlled vocabularies will be provided by three SKOS based thesauri:

 * The [Thesaurus for the Social Sciences](http://www.gesis.org/en/services/research/thesauri-und-klassifikationen/social-science-thesaurus/) ([The Soz](http://www.semantic-web-journal.net/sites/default/files/swj279_2.pdf)).
 * The [IPTC](https://iptc.org/) media topics [thesaurus](https://iptc.org/standards/media-topics/), a 1100-term taxonomy with a focus on categorizing text. The Media Topics vocabulary can be viewed on the [IPTC Controlled Vocabulary server]( http://cv.iptc.org/newscodes/mediatopic).
 * The [STW Thesaurus for Economics](http://zbw.eu/stw/versions/latest/download/about.en.html).
 
Three options exist for performing the keyword linking:
 
 * Dictionary Matching Transformer,
 * Stanbol Entityhub Linking Engine
 * Stanbol FST Linking Engine 

The Stanbol linking engines would be accessed through the Stanbol FP3 Transformer. In the event that the Dictionary Matching Transformer cannot cope with the intended workload, Stanbol is a scalable alternative. In order for Stanbol to work with these custom vocabularies, the SKOS versions of the thesauri must be imported into a Jena TDB quad store, and indexed by the EntityHub Indexing Tool to create an Apache Solr index.


####8: Matching against DBpedia

Some entities are not covered by the GND, or the subject terminologies. In order to find additional related concepts in DBpedia and related Wiki pages in Wikipedia, we propose trying to find them using alternative keyword linking services. Two candidate services are [dataTXT](http://dandelion.eu/datatxt/) and [DBpedia Spotlight](https://github.com/dbpedia-spotlight/dbpedia-spotlight/wiki). Both are available as Stanbol enhancement engines, accessible through the FP3 Stanbol Enhancer Transformer.

dataTXT is a named entity extraction & linking service that, given a plain text, gives back a set of entities found in the text and links to corresponding entries in Wikipedia. It also optionally returns the entity type from DBpedia. 

dataTXT performs very well even on short texts, on which many other similar services do not. It is based on co-references of entities, it does not use any NLP feature. Even if the input texts are not complete sentences, we expect it to return valid results. For this reason it is our preferred service for this step.

DBpedia Spotlight provides a fallback option. However, we feel it is unlikely to produce better results than dataTXT. Should we opt to try this option, we will configure our own DBpedia Spotlight service.

**TO DO:** More technical details of this processing step required

[Q->Adrian] Re: the use case example for step 8: "Cantata on the death of a comrade for singing..."

- Where is this text from?
- What is the B3Kat URI of this entity?
- Does this entity not have a corresponding GND ID?

##Data Consumption

The large scale validation should include examples of consuming the enriched/interlinked data. For this project, we believe it is sufficient to provide example SPARQL queries illustrating how the Linked Data might be consumed, or at most create a simple user interface.

TO DO: Sample SPARQL queries.

##Validation Metrics

TO DO

## Validation Results

The validation results will be provided once available.

## Conclusions and Future Work ##

TO DO

## References ##

TO DO

## Appendix A - GND RDF Description of Hanns Eisler

Below is the description of Hanns Eisler extracted from the GND dataset, illustrating a typical GND RDF description.

    <rdf:Description rdf:about="http://d-nb.info/gnd/118529692">
    	<rdf:type rdf:resource="http://d-nb.info/standards/elementset/gnd#DifferentiatedPerson" />
    	<foaf:page rdf:resource="http://de.wikipedia.org/wiki/Hanns_Eisler" />
    	<owl:sameAs rdf:resource="http://dbpedia.org/resource/Hanns_Eisler" />
    	<owl:sameAs rdf:resource="http://viaf.org/viaf/19865132" />
    	<gndo:gndIdentifier>118529692</gndo:gndIdentifier>
    	<gndo:oldAuthorityNumber>(DE-588a)118529692</gndo:oldAuthorityNumber>
    	<gndo:oldAuthorityNumber>(DE-588a)185686249</gndo:oldAuthorityNumber>
    	<gndo:oldAuthorityNumber>(DE-588a)139523375</gndo:oldAuthorityNumber>
    	<gndo:oldAuthorityNumber>(DE-588a)134366263</gndo:oldAuthorityNumber>
    	<gndo:oldAuthorityNumber>(DE-101c)310056268</gndo:oldAuthorityNumber>
    	<gndo:oldAuthorityNumber>(DE-588c)4014114-7</gndo:oldAuthorityNumber>
    	<gndo:variantNameForThePerson>Eisler, ...</gndo:variantNameForThePerson>
    	<gndo:variantNameEntityForThePerson rdf:parseType="Resource">
    		<gndo:forename>...</gndo:forename>
    		<gndo:surname>Eisler</gndo:surname>
    	</gndo:variantNameEntityForThePerson>
    	<gndo:variantNameForThePerson>Eissler, Hanns</gndo:variantNameForThePerson>
    	<gndo:variantNameEntityForThePerson rdf:parseType="Resource">
    		<gndo:forename>Hanns</gndo:forename>
    		<gndo:surname>Eissler</gndo:surname>
    	</gndo:variantNameEntityForThePerson>
    	<gndo:variantNameForThePerson>Eisler, Hans</gndo:variantNameForThePerson>
    	<gndo:variantNameEntityForThePerson rdf:parseType="Resource">
    		<gndo:forename>Hans</gndo:forename>
    		<gndo:surname>Eisler</gndo:surname>
    	</gndo:variantNameEntityForThePerson>
    	<gndo:variantNameForThePerson>Eisler, Johannes</gndo:variantNameForThePerson>
    	<gndo:variantNameEntityForThePerson rdf:parseType="Resource">
    		<gndo:forename>Johannes</gndo:forename>
    		<gndo:surname>Eisler</gndo:surname>
    	</gndo:variantNameEntityForThePerson>
    	<gndo:preferredNameForThePerson>Eisler, Hanns</gndo:preferredNameForThePerson>
    	<gndo:preferredNameEntityForThePerson rdf:parseType="Resource">
    		<gndo:forename>Hanns</gndo:forename>
    		<gndo:surname>Eisler</gndo:surname>
    	</gndo:preferredNameEntityForThePerson>
    	<gndo:familialRelationship rdf:resource="http://d-nb.info/gnd/116435410" />
    	<gndo:familialRelationship rdf:resource="http://d-nb.info/gnd/124362214" />
    	<gndo:familialRelationship rdf:resource="http://d-nb.info/gnd/118691392" />
    	<gndo:familialRelationship rdf:resource="http://d-nb.info/gnd/118681850" />
    	<gndo:familialRelationship rdf:resource="http://d-nb.info/gnd/1054173877" />
    	<gndo:professionOrOccupation rdf:resource="http://d-nb.info/gnd/4032009-1" />
    	<gndo:professionOrOccupation rdf:resource="http://d-nb.info/gnd/4040841-3" />
    	<gndo:playedInstrument rdf:resource="http://d-nb.info/gnd/4030982-4" />
    	<gndo:playedInstrument rdf:resource="http://d-nb.info/gnd/4057587-1" />
    	<gndo:gndSubjectCategory rdf:resource="http://d-nb.info/standards/vocab/gnd/gnd-sc#14.4p" />
    	<gndo:geographicAreaCode rdf:resource="http://d-nb.info/standards/vocab/gnd/geographic-area-code#XA-DE" />
    	<gndo:geographicAreaCode rdf:resource="http://d-nb.info/standards/vocab/gnd/geographic-area-code#XD-US" />
    	<gndo:geographicAreaCode rdf:resource="http://d-nb.info/standards/vocab/gnd/geographic-area-code#XA-AT" />
    	<gndo:placeOfBirth rdf:resource="http://d-nb.info/gnd/4035206-7" />
    	<gndo:placeOfDeath rdf:resource="http://d-nb.info/gnd/4005728-8" />
    	<gndo:placeOfActivity rdf:resource="http://d-nb.info/gnd/4042011-5" />
    	<gndo:placeOfExile rdf:resource="http://d-nb.info/gnd/4078704-7" />
    	<owl:sameAs rdf:resource="http://www.filmportal.de/person/18AC4FE0900B4565A8D821ED5F6A175E" />
    	<gndo:gender rdf:resource="http://d-nb.info/standards/vocab/gnd/Gender#male" />
    	<gndo:dateOfBirth rdf:datatype="http://www.w3.org/2001/XMLSchema#date">1898-07-06</gndo:dateOfBirth>
    	<gndo:dateOfDeath rdf:datatype="http://www.w3.org/2001/XMLSchema#date">1962-09-06</gndo:dateOfDeath>
    </rdf:Description>

##Appendix B - Overview Notes on Apache Stanbol

This appendix provides a brief overview of the main components of Apache Stanbol relevant to the LKC use case, with the aim of supplying some background context to the technical points raised in the main document, for readers unfamiliar with Stanbol.

Within Apache Stanbol, the Stanbol Enhancer provides the core services required for tag extraction and suggestion.

As described in the D3.1 Deliverable Report {TO DO: Link}, the Stanbol Enhancer provides two main components: enhancement engines and enhancement chains; both accessed by the FP3 platform through a Stanbol FP3 Transformer.

A typical Stanbol enhancement workflow is depicted below:

![Typical Stanbol enhancement workflow](https://stanbol.apache.org/docs/trunk/enhancementworkflow.png)

### Enhancement Engines

The Enhancer component together with its Enhancement Engines provides you with the ability to post content to Apache Stanbol and get suggestions for possible entity annotation in return. The enhancements are provided via natural language processing, metadata extraction and linking named entities to public or private entity repositories.

Numerous enhancement engines are supported, spanning several types, including:

* Preprocessing
  * perform content type detection and text extraction from various document formats;
* NLP
  * process textual content performing language detection, sentence detection, tokenizing, POS tagging, NER;
* Linking
  * suggest entities for features present in the parsed content. An entity in this context is a uniquely identified resource. Typically it provides, or links to, further information such as the entity type and a description.
  
Linking engines of relevance to the LKC use case are:

* Entityhub Linking Engine
  * provides a configured instance of a Stanbol EntityHub;
  * links entities managed by the Entityhub, including entities from remote sites;
  
* FST Linking Engine
  * links entities indexed by a Solr index;
  * provides better linking performance than the Entityhub Linking Engine;
  
* dataTXT Enhancement Engine
  * TO DO: Check
  * integrates dataTXT with the Stanbol Enhancer
  * includes NLP, entity linking and entity disambiguation using a remote DBpedia service instance as the knowledge base

###Types of Entity Linking

Stanbol distinguishes two possible ways to recognize entities in a custom vocabulary:

* Named entity linking;
* Entity linking (aka Keyword linking).

Either approach forms the semantic lifting phase (i.e. the process of associating content items with suitable semantic objects as metadata to turn “unstructured” content items into semantic knowledge resources) in a Stanbol enhancement chain.

 While Named Entity Linking only considers named entities detected by NER, Entity Linking works on tokens. Entity Linking only requires correct tokenization of the text. Named Entity Linking use the type information provided by NER, but can only support entity types supported by the underlying NER models (persons, organizations and places). Entity Linking does not have this restriction, but has the disadvantage that entity lookups against a controlled vocabulary are based only on label similarities.

To use Entity Linking with a custom vocabulary, users need to configure an instance of a Stanbol Entityhub Linking Engine or a FST Linking engine.

###Working with Custom Vocabularies

Stanbol's support for custom vocabularies enables applications to detect and work with concepts from a specific domain - in this case the IPTC taxonomy, The Soz and the STW Thesaurus for Economics. For the LKC use case, we propose managing custom vocabularies by using a Stanbol Referenced Site with a full local index. This entails:

* building full local indexes with the EntityHub Indexing tool;
* importing these indexes into Apache Stanbol;
* configuring the Stanbol Enhancer to make use of the indexed and imported vocabularies.

### Stanbol EntityHub

The [Entityhub](https://stanbol.apache.org/docs/trunk/components/entityhub) is the Stanbol component responsible for providing the information about entities relevant to the users domain.  It allows one to manage local entities as well as import entities from remote knowledge bases (aka referenced sites) such as DBpedia. Entities from referenced sites can be cached locally and indexed for improved performance.

###EntityHub Indexing Tool

The indexing tool creates an Apache Solr index of RDF files, e.g. a SKOS export of a thesaurus. RDF files to be indexed are imported to a Jena TDB triple store which is used as the source for the indexing.