<a name="LTVFPP"></a> 
#Large-scale Technical Validation of Fusepool P3 Platform 
###Deliverable D5.4

![enter image description here](https://avatars0.githubusercontent.com/u/5859504?v=3&s=200)


###Contents
- [Large-scale Technical Validation of Fusepool P3 Platform](#LTVFPP)      
    - [Document History](#DocumentHistory)
    - [Document Information](#DocumentInformation)
    - [Document Context Information](#DocumentContextInformation)
    - [Quality Assurance / Review](#QualityAssurance)
    - [Official Citation](#OfficialCitation)
    - [Copyright](#Copyright)
    - [Acronyms and Abbreviations](#Acronyms)
- [Executive Summary](#ExecutiveSummary)
- [Introduction](#Introduction)
- [Library Keyword Clustering (LKC)](#LKC)
  - [Key Datasets](#KeyDatasets)
    - [DNB Dataset](#DNBDataset)
    - [B3Kat Dataset](#B3KatDataset)
    - [DBpedia Dataset](#DBpediaDataset)
  - [Initial Dataset Connections](#InitialDatasetConnections)
    - [B3Kat - GND](#B3KatGND)
    - [GND - DBpedia](#GNDDBpedia)
  - [Primary Use Case Goals](#PrimaryUseCaseGoals)
  - [Broad Approach](#BroadApproach)
- [Test Infrastructure](#TestInfrastructure)
  - [Estimated Dataset Sizes](#EstimatedDatasetSizes)
  - [Software](#Software)
    - [FP3 Platform Deployment](#FP3PlatformDeployment)
    - [Storage Layer](#StorageLayer)
  - [Hardware](#Hardware)
    - [Memory Requirements](#MemoryRequirements)
- [Data Transformation](#DataTransformation)
  - [Source Dataset Loading](#SourceDatasetLoading)
  - [Use Case Task Details](#UseCaseTaskDetails)
    - [1: Make a graph per GND subject for each B3Kat record](#UseCaseTaskDetails1)
    - [2: Get basic information about each GND subject](#UseCaseTaskDetails2)
    - [3: Get additional GND subject data from DBpedia](#UseCaseTaskDetails3)
    - [4: Get co-occurring concepts from B3Kat](#UseCaseTaskDetails4)
    - [5: Get texts from other B3Kat documents referencing the concept](#UseCaseTaskDetails5)
    - [6: Perform NER on selected document texts](#UseCaseTaskDetails6)
    - [7: Match thesauri concepts to each GND](#UseCaseTaskDetails7)
    - [8: Matching against DBpedia](#UseCaseTaskDetails8)
- [Data Consumption](#DataConsumption)
- [Validation Metrics](#ValidationMetrics)
- [Validation Results](#ValidationResults)
- [Conclusions and Future Work](#ConclusionsandFutureWork)
- [Appendix A: GND RDF Description of Hanns Eisler](#AppendixA)
- [Appendix B: Overview Notes on Apache Stanbol](#AppendixB)
  - [Information Extraction for the LKC Use Case](#AppendixBLELUC)
  - [Enhancement Chain](#AppendixBEC)
  - [Enhancement Engines](#AppendixBEE)
  - [Working with Custom Vocabularies](#AppendixBWCV)
  - [Stanbol EntityHub](#AppendixBSEH)
- [Appendix C: Dataset Sources](#AppendixC)
   - [B3Kat](#AppendixCB3Kat)
   - [GND](#AppendixCGND)
   - [STW](#AppendixCSTW)
   - [TheSoz](#AppendixCTheSoz)
   - [IPTC](#AppendixCIPTC)
   - [EuroVoc](#AppendixCEuroVoc)

----------
<a name="DocumentHistory"></a> 
#### Document History ####

| Ver. | Name | Date | Remark |
| :---: | :--- | :---: | :--- |
| v0.0 | Carl Blakeley, Milos Jovanovik | 08.06.2015 | Initial draft |
| v0.1 | Milos Jovanovik, Carl Blakeley, Adrian Gschwend, Reto Gmür, Rupert Westenthaler, Johannes Hercher | 25.06.2015 | Pre-review version |
| v0.2 | Milos Jovanovik, Carl Blakeley, Adrian Gschwend, Reto Gmür, Rupert Westenthaler | 29.06.2015 | Reviewed version |

<a name="DocumentInformation"></a> 
#### Document Information ####

- Deliverable Nr Title: D5.4 Large-scale technical validation of Fusepool P3 platform
- Lead: Milos Jovanovik (OGL)
- Authors: Milos Jovanovik, Carl Blakeley (OGL), Adrian Gschwend (BUAS), Reto Gmür (BUAS), Rupert Westenthaler (SRFG), Johannes Hercher (Freie Universität Berlin)
- Publication Level: Public

<a name="DocumentContextInformation"></a> 
#### Document Context Information ####

 - Project (Title/Number): Fusepool P3 (609696)
 - Work Package / Task: WP5 / T5.6
 - Responsible person and project partner: Milos Jovanovik (OGL)

<a name="QualityAssurance"></a> 
#### Quality Assurance / Review ####

- 1st reviewer: Jakob Frank (SRFG)
- 2nd reviewer: Luigi Selmi (BUAS)

<a name="OfficialCitation"></a> 
#### Official Citation ####
Fusepool-P3-D5.4

<a name="Copyright"></a> 
#### Copyright ####

This document contains material, which is the copyright of certain Fusepool P3 consortium parties. This work is licensed under the Creative Commons Attribution 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.

<a name="Acronyms"></a> 
#### Acronyms and Abbreviations ####

| Acronym | Description |
| --- | :--- |
| DoW | Description of Work |
| DDC | Dewey Decimal Classification |
| ECC | Error correcting code |
| NER | Named entity recognition |
| NEL | Named entity linking |
| NLP | Natural language processing |
| FP3 | Fusepool P3 |
| FAM | Fusepool Annotation Model |
| FST | Finite state transducer |
| GND | Gemeinsame Normdatei (Integrated Authority File) |
| LLC | Library of Congress Classification |
| Lccn| Library of Congress classification number|
| LDP | Linked Data Plattform |
| POS | Part of speech |
| SSD | Solid-state drive |

<a name="ExecutiveSummary"></a> 
## Executive Summary ##

This document details the work performed and planned as part of Deliverable D5.4 / Task 5.6 "Large-scale technical validation of the Fusepool P3 platform".

**Status, June 2015:** *At the current time this is an **interim report** detailing our goals and approach to large scale technical validation of the FP3 Linked Data Platform. The validation work is currently on-going. It is our intention to include the validation test results themselves in a revised version of this report once testing is complete. Regrettably the consortium was unable to commence Task 5.6 in March 2015 (month 15 of the project), as originally planned in the DoW, with work on this task not starting until the beginning of June 2015. The main reasons for this delay were not having sufficient stakeholders engaged in the project and as a consequence the lack of a suitable use case in March 2015 for large-scale testing. Since then, WP1 has engaged an additional stakeholder, Johannes Hercher of the Freie Universität Berlin, who has provided the core use case.*

The large scale testing is based on a central use case, Library Keyword Clustering, in which the main processing steps are:

* Aggregation of book subject information from library catalogues, authority files and DBpedia
* Analysis of coocurrences of book subject concepts and keyword clustering
* Analysis of the words/terms used in conjunction with a subject identifier (word contexts) to find related entities using Named Entity Recognition (NER) and Named Entity Linking (NEL)

The aim is to demonstrate large scale aggregation and transformation of information distributed across various locations, through a unified rerunnable workflow provided by the FP3 platform and which requires no end-user intervention once configured. Automated Linked Data generation and publishing through FP3 would be a significant improvement on the currently cumbersome, labour intensive manual subject indexing by librarians.

The Library Keyword Clustering (LKC) use case centres on the interlinking of three large datasets:  two library catalogues, the [B3Kat](https://lod.b3kat.de/) catalogue and the [GND Authority File of the German National Library](http://www.dnb.de/EN/Standardisierung/GND/gnd.html),  and [DBpedia](http://dbpedia.org). 

The seed dataset, B3Kat, contains approximately 26 million bibliographic records. The three core datasets combined total around 2 billion triples. We estimate that the resulting data corpus after enrichment will be approaching 3 billion triples.

<a name="Introduction"></a> 
## Introduction ##

The DoW {TO DO: Link} for the Fusepool P3 project describes the main task of this deliverable as: 

*"T5.6 Technical validation of Fusepool Linked Data Platform: implementation will be tested through the large-scale real life use cases defined in WP1 to ensure that an industry-strength Linked Data platform is created...".*

In selecting use cases for large scale testing, our choice was influenced by two criteria: 

 - the test data should be sufficiently large to adequately test the ability of the FP3 platform to handle large data volumes;
 - processing of the test data should exercise as much of the platform as possible.

The use cases driving the development of the FP3 Platform have been defined as part of WP1 and are described in the Deliverable 1.1 report ["D1.1 Use cases and data specified, data modelled and prepared"](https://github.com/fusepoolP3/p3-wp1-deliverables/blob/master/d11-deliverable.md). From these, we selected one use case to form the basis of the large scale testing of the platform:

Large Scale Validation Use Case: Library Keyword {TODO: Link?} Clustering](https://github.com/fusepoolP3/p3-wp1-deliverables/blob/master/d11-deliverable.md#large-scale-validation-use-case-library-keyword-clustering)

This use case focuses on exercising the bulk import and bulk transformation features of the platform, along with the ability to handle large datasets.

<a name="LKC"></a> 
## Library Keyword Clustering (LKC) 

The "Linked Keyword Clustering" (LKC) use case is described in detail in the WP1 [Deliverable 1.1 report](https://github.com/fusepoolP3/p3-wp1-deliverables/blob/master/d11-deliverable.md#large-scale-validation-use-case-library-keyword-clustering)]. For convenience, we provide an overview below.

<a name="KeyDatasets"></a> 
### Key Datasets
The LKC use case centres on the interlinking of three large RDF datasets:  two library catalogues - the [GND Authority File of the German National Library](http://www.dnb.de/EN/Standardisierung/GND/gnd.html) and the [B3Kat](https://lod.b3kat.de/) catalogue - and [DBpedia](http://dbpedia.org).

<a name="DNBDataset"></a> 
####DNB Datasets
The German National Library (DNB) is dedicated to collect and describe all contents published in Germany in order to make cultural heritage accessible in the long term. The [Linked Data Service of the German National Library](http://www.dnb.de/EN/Service/DigitaleDienste/LinkedData/linkeddata_node.html) has made their entire Datasets (i.e. [DNB catalogue](http://datendienst.dnb.de/cgi-bin/mabit.pl?cmd=fetch&userID=opendata&pass=opendata&mabheft=DNBTitel.ttl.gz) and the national [Authority File (GND)](http://datendienst.dnb.de/cgi-bin/mabit.pl?cmd=fetch&userID=opendata&pass=opendata&mabheft=GND.ttl.gz) ) available as Linked Data.

###### Catalogue
The catalogue contains bibliographic descriptions both for print and electronic materials. The DNB catalogue contains approximately 11.953.014 records (as of and is available free of charge through the [Linked Data Service of the German National Library](http://www.dnb.de/EN/Service/DigitaleDienste/LinkedData/linkeddata_node.html)The Catalogue is roughly {TODO}: exact size in GB.

###### GND 
The [Integrated Authority File](http://en.wikipedia.org/wiki/Integrated_Authority_File) (German: Gemeinsame Normdatei) or GND is the national authority file for the unique identification of entities (e.g. personal names, subject headings, corporate bodies,…). As a controlled vocabulary it is used for organization of alternative labels for entities, as well as their semantic relations. According to its broad application the GND is the most important vocabulary to describe publications' contents in Germany and German-speaking Europe. It is mainly used by libraries, but increasingly also by archives and museums. GNDs is hosted by the German National Library (Deutsche National Bibliothek) and maintained in cooperation with various library networks.
According to [LOD Stats](http://stats.lod2.eu/rdfdocs/157) it contains 124.402.922 Triples about 36.575.972 Entities organized in 48 Classes and 189 Properties. It has a size of acround 10GB (decompressed). 

<a name="B3KatDataset"></a> 
####B3Kat Dataset
[B3Kat](http://www.b3kat.de/) is the "common union catalogue" of [BVB](http://www.bib-bvb.de/) (Bavarian Library Network) and [KOBV](http://www.kobv.de/) (Cooperative Library Network Berlin-Brandenburg). The [B3Kat Linked Open Data Service](https://lod.b3kat.de/) provides serveral cataloguing services and aggregates library data of 180 academic libraries in Bavaria, Berlin and Brandenburg, describing roughly 26 million titles. 

The [B3Kat RDF data dump](https://lod.b3kat.de/download) contains about 890 million RDF triples in 7.7GB of data.

Data contained in the set is about all publications (Text-Books, Dissertations, Journals, Maps, Audio recordings, etc.) aquired by the 180 member libraries. Opposing to the DNB catalogue (see above), these publications don't have to be published in Germany. Moreover the dataset contains descriptions about libraries and their locations. 
A sample record can be seen here: https://lod.b3kat.de/page/title/BV023184541
{TO DO}: Add some more information about the Data structure ?!

<a name="DBpediaDataset"></a> 
####DBpedia Dataset
The [DBpedia](http://dbpedia.org) dataset describes 4.58 million entities, including 1,445,000 persons, 735,000 places, and 241,000 organizations. The data set features labels and abstracts for these entities in up to 125 different languages; 25.2 million links to images and 29.8 million links to external web pages. It consists of 3 billion RDF triples, out of which 580 million were extracted from the English edition of Wikipedia, 2.46 billion were extracted from other language editions. DBpedia is connected with other Linked Datasets by around 50 million RDF links. [DBpedia dumps](http://wiki.dbpedia.org/Downloads) in 119 languages are available on the DBpedia download server.

{TO DO}: Are these numbers correct? 3 billion DBpedia alone? There is another number later in this document when we calculate the total amount of triples.


<a name="InitialDatasetConnections"></a> 
### Initial Dataset Connections

At the outset, before any interlinking by the FP3 framework, minimal connections existed between the source datasets. These minimal base connections, which provided the initial basis for richer interlinking and enrichment, are outlined below.

<a name="B3KatGND"></a> 
#### B3Kat - GND
The B3Kat dataset links to the GND corpus through GND IDs held as `dcterms:subject` values. 

The home page of the [B3Kat Linked Open Data Service](https://lod.b3kat.de/) includes a link for displaying an [example title](https://lod.b3kat.de/example). From this, it can be seen that the `dcterms:subject` property contains a number of subject identifiers, using several classification schemes include [GND](http://www.dnb.de/EN/Standardisierung/GND/gnd.html), [DDC](http://en.wikipedia.org/wiki/Dewey_Decimal_Classification) and [RVK](https://www.uni-erfurt.de/en/bibliothek/library-erfurt/research/classification-rvk/). The LKC use case makes reference to Hanns Eisler's "Deutsche Sinfonie" <https://lod.b3kat.de/page/title/BV001289953> as a central example - again the `dcterms:subject` property uses subject identifiers spanning the GND, DDC, [LLC](http://en.wikipedia.org/wiki/Library_of_Congress_Classification) and RVK schemes.

<a name="GNDDBpedia"></a> 
#### GND - DBpedia

The GND dataset includes links to DBpedia resource descriptions for many items, through `owl:sameAs` properties. For example, the GND entry for Hans Eisler includes:

    <rdf:Description rdf:about="http://d-nb.info/gnd/118529692">
        <rdf:type 
        rdf:resource="http://d-nb.info/standards/elementset/gnd#DifferentiatedPerson" />
        <foaf:page       rdf:resource="http://de.wikipedia.org/wiki/Hanns_Eisler" />
        <owl:sameAs rdf:resource="http://dbpedia.org/resource/Hanns_Eisler" />
        ...
        <gndo:surname>Eisler</gndo:surname>
        ...
    </rdf:Description>
 
<a name="PrimaryUseCaseGoals"></a> 
###Primary Use Case Goals

Beyond providing a basis for large scale testing of the FP3 platform, the main goals from the user's perspective of this application is to make GNDs identifiers more usable, and to enhance retrieval of publications in libraries. In detail this leads to the following sub-goals:

 - **Add context around a GND concept, and identifying related concepts**:
  - Cluster GND identifiers based on their degree of coocurrence in library records. 
  - Aggregate information about GND identifiers located in other datasets, i.e. textual descriptions, geo-information, images from DBpedia.
 - **Enrich GND concepts by harnessing FP3 NER and dictionary matching capabilities**: 
  - Find co-occurring entities (places, persons, organizations ...) for each subject concept. For instance it is analyzed which places are mentioned in the title text of books about Hanns Eisler? 
  - Count-Of-Occurence-Analysis for mentioned entities, e.g. to create tag-clouds.
 - **Enrich library records**: Recognize identifiers from catalogue records using the FP3 Dictionary Matcher, to identify text strings in the library records that are not yet linked to their corresponding GND identifier.
 
<a name="BroadApproach"></a> 
### Broad Approach

- Load the GND, B3Cat and DBpedia datasets into quad storage, making them available for querying via SPARQL.
- Use CONSTRUCT queries to create initial graphs, one graph per GND.
- From these graphs, generate approximately 25 million files containing mash-ups of the source RDF data, which will then be posted to a transforming LDP container.
- Once in LDP, the linked transformer triggers entity extraction and NER on book titles.
 
{TO DO}: To Technical for an Overview (e.g. use CONSTRUCT queries) ... focus on the description of the necessary steps. e.g. Import the RDF, Preprocess the RDF according to the use-case requirements. Enrich the RDF using the Fusepool Platform. Host the enriched Data using the Fusepool Platform (SPARQL and LDP).

<a name="TestInfrastructure"></a> 
## Test Infrastructure

<a name="EstimatedDatasetSizes"></a> 
### Estimated Dataset Sizes

The source datasets total approximately 2 billion triples:

| Source dataset | Approximate size |
| :-- | :-- |
| GND | 124 million triples |
| B3Cat | 890 million triples |
| Dbpedia | 1 billion triples (assuming the full dataset and associated ontologies are loaded) |

{TO DO}: These numbers do not correspond to the numbers above in the overview. Either we need to say that we use a subset of them or we correct it accordingly.

We estimate the resultant transformed aggregated data will comprise under 1 billion triples - a Turtle mockup of the desired output by the end users suggests that there will be ~200 triples per subject. Assuming the source data covers approximately 3 million authors, the output data will be of the order of 600 million triples held in LDP storage.

The estimated total data corpus size of 3 billion triples, together with past experience of the core software components to be exercised, was then used as the basis for specifying the test platform hardware.

<a name="Software"></a> 
###Software

<a name="FP3PlatformDeployment"></a> 
#### FP3 Platform Deployment

For ease of deployment, the FP3 test platform will be composed from Docker images.

 -   [FP3 Platform reference implementation - Docker image details]( https://github.com/fusepoolP3/p3-platform-reference-implementation)
 - [VOS - Docker image details](https://github.com/fusepoolP3/virtuoso-docker)
 
<a name="StorageLayer"></a> 
#### Storage Layer

Although Marmotta has provided the RDF storage for much of the FP3 development, Virtuoso Open Source Edition (VOS) was chosen as the storage layer, to provide the quad store, LDP server and SPARQL endpoints. Evaluations tests have found Virtuoso to be scalable to 10 billion+ triples. Virtuoso Commercial Edition was not considered necessary for the calculated use case dataset sizes. VOS supports multiple cores, so a single server instance running on a single machine is suitable. The GND, B3Cat and DBpedia datasets will be loaded directly into the Virtuoso quad store.

<a name="Hardware"></a> 
### Hardware

The test platform comprises a dedicated server, not a virtual machine, with the following specification:

 - 2 x Intel Xeon E5 2620V2, 2 x (6 x 2.10 GHz)
 - 128 GB buffered ECC RAM
 - 1000 GB SSD (Samsung 840 EVO)
 - Ubuntu 14.04

<a name="MemoryRequirements"></a> 
####Memory Requirements

Based on the anticipated size of the datasets and generated data, we estimated the amount of RAM required to be roughly as follows:

 - Virtuoso 7 typically requires 10GB per billion triples (10 bytes per quad), depending on how well the particular datasets compress for storage. 32GB RAM for Virtuoso was considered sufficient for ~3 billion triples. In general, memory is more important than disk storage, as once the database is 'warm', i.e. the database working set is in memory, then disk access is minimized.
 - Stanbol's memory requirements were estimated to be ~4GB. For creating Solr indexes with the Entityhub Indexing Tool, Jena TDB will be used as the source for indexing. Jena TDB is not required at runtime. The entity labels are held in memory, encoded in a FST (~300MB for all DBpedia's English labels).
 - Other Fusepool Transformers memory requirements are limited to in-memory caching of currently processed items. Their memory requirement will be far below 1GB.

<a name="DataTransformation"></a> 
##Data Transformation
<a name="SourceDatasetLoading"></a> 
### Source Dataset Loading

A prerequisite for transforming all this data was that the source datasets be loaded into the test LDP server. Virtuoso incorporates a bulk loader for importing large RDF datasets.  Although B3Kat and DBpedia provide public SPARQL endpoints (the German National Library does not make a SPARQL endpoint for GND catalogue), it was not thought feasible (or responsible) to rely on these endpoints for such large scale processing.

<a name="UseCaseTaskDetails"></a> 
### Use Case Task Details

The use case description provided by the WP1 Deliverable 1.1 report lists the main processing steps necessary to provide the desired enrichment and interlinking. These are re-iterated again below, but with a focus on the technical details of realizing the data transformations.

Some of the use case tasks, primarily preparation of the input data in tasks 1 - 5, are to be performed by the application stakeholders (primarily Johannes Hercher, Free University Berlin). Tasks 6 - 8 will be performed by the FP3 platform itself and constitute the core of the validation testing.
 
<a name="UseCaseTaskDetails1"></a> 
####1: Make a graph per GND subject for each B3Kat record

Approximately half of B3Kat's 26 million bibliographic records about books include links to between 1 and 10 related GND subject identifiers. 

See for example Hanns Eisler’s Deutsche Sinfonie https://lod.b3kat.de/page/title/BV001289953. Using the [B3Kat SPARQL endpoint](https://lod.b3kat.de/doc/de/sparql-endpoint/):

     SELECT * WHERE { 
       <http://lod.b3kat.de/title/BV001289953> 
         dc:title ?title ;
         dct:subject ?subject 
         filter regex(str(?subject), "d-nb.info/gnd")
     }
    
returns
    
| title | subject |
| :-- | :-- |
| Hanns Eislers "Deutsche Sinfonie" | http://d-nb.info/gnd/118529692 |
| Hanns Eislers "Deutsche Sinfonie" | http://d-nb.info/gnd/4222905-4  |

For each GND ID listed as a `dct:subject` in a B3Kat record, a graph will be created by a CONSTRUCT query, using the GND ID as the graph URI.

<a name="UseCaseTaskDetails2"></a> 
####2: Get basic information about each GND subject

B3Kat contains only GND IDs and no additional information from the GND authority file is available. However, this information is available in the GND RDF dumps. For instance, alternative labels, geo information, semantic relations (broader, narrower, related), subject categories, mappings to Wikipedia and DBpedia. (See also Appendix A for an example of a typical entry from the GND dataset.)

Each graph created in step 1 will be augmented with information about the subject concept extracted from the subject description in the GND dataset.

<a name="UseCaseTaskDetails3"></a> 
####3: Get additional GND subject data from DBpedia

Many GND concepts have a link to DBpedia.  See Appendix A for an example;  the description of Eisler includes:

    <owl:sameAs rdf:resource="http://dbpedia.org/resource/Hanns_Eisler" /> 

This link will be used to fetch additional data, such as: `foaf:depiction`, `dbpedia-owl:thumbnail`, `dc:description`, `rdfs:comment`, `dbpedia-owl:abstract`, `dcterms:subject`, etc., to augment each base graph of step 1.

<a name="UseCaseTaskDetails4"></a> 
####4: Get co-occurring concepts from B3Kat

For a given GND-ID we can query co-occurring subject headings (GND-IDs, LcSH) and classification codes (RVK, DDC, Lccn, ...) from records of the B3Kat at its [SPARQL endpoint](https://lod.b3kat.de/doc/de/sparql-endpoint/).

The goal is to compute the co-occurrence of a GND to other GNDs and subject identifiers from other subject vocabularies (RVK, DDC, Lccn). The co-occurrence of concepts can be computed directly and attached to the GND graphs from step 1. This will result in many additional triples but we may limit the addition of co-occurrence relation triples to a particular GND graph (e.g. for Hanns Eisler) at a certain degree of co-occurrence e.g. 5-10%.

We can retrieve identifiers that appear in the same context/record with a GND-ID. All identifiers are marked as co-occurrent and added to the GND-ID graph (cf. step 1): 

    SELECT ?record ?cosubjectLabel ?cosubjectUri ?colccn ?coisbn ?cocreator
    WHERE { 
    ?record ?p <http://d-nb.info/gnd/118529692> . #- Hanns Eisler
      OPTIONAL { ?s dc:subject ?cosubjectLabel }
      OPTIONAL { ?s dct:subject ?cosubjectUri }
      OPTIONAL { ?s bibo:lccn ?colccn }
      OPTIONAL { ?s bibo:isbn ?coisbn }
      OPTIONAL { ?s marcrel:aut ?cocreator }
    }
   
Additionally, it is counted how often a concept co-occurs with the given GND-ID to compute a the distance measure. The distance `d` of a base concept `Cb` (i.e. Hanns Eisler) to a coocurrent concept `Cx` is computed as the percentage of the total record count having `Cb`.

    SELECT (count(*) AS ?totalNumOfRows)
    WHERE { 
    ?record ?p <http://d-nb.info/gnd/118529692> ;  #- Hanns Eisler
            dct:subject ?cosubjectUri .
          }
    # Returns => 2279

Using the total count of rows for a certain GND-ID we compute `d` augument the GND-ID graph for `Hanns Eisler` with this information.

    SELECT ?cosubjectUri (COUNT(?record) AS ?recordCount)
    WHERE { 
    	 ?record ?p <http://d-nb.info/gnd/118529692> ;
            dct:subject ?cosubjectUri . #- Hanns Eisler
          }
          GROUP By ?cosubjectUri
          ORDER BY DESC(?recordCount)
          # returns:  
          #<http://lod.b3kat.de/ssg/9.2>		595 * 100 / 2279 => 26% 
          #<http://lod.b3kat.de/rvk/LU91850>	226 * 100 / 2279 => 10%
          #<http://d-nb.info/gnd/118529692>	165 * 100 / 2279 => 7%

<a name="UseCaseTaskDetails5"></a> 
####5: Get texts from other B3Kat documents referencing the concept

For a given GND concept, we search for other documents which reference the concept in some way and extract their title and subtitle as the basis for NER.

e.g. Find the title and subtitle of documents which reference Hanns Eisler:

    PREFIX isbd: <http://iflastandards.info/ns/isbd/elements>

    SELECT DISTINCT ?s ?title ?subtitle
     WHERE { 
      ?s ?p <http://d-nb.info/gnd/118529692> ; # - Hanns Eisler 
     dc:title ?title ;
     optional { ?s isbd:P1006 ?subtitle }
    } LIMIT 500
    
<a name="UseCaseTaskDetails6"></a> 
####6: Perform NER on selected document texts

At this point, the graphs created and populated by steps 1 - 5 provide the base context for enrichment and interlinking by the FP3 platform. By some ETL process external to the platform and orchestrated by the end-user, each graph (1 per GND concept) will be used to generate an RDF document (of somewhere between 100 - 1000 triples) which will be deposited in a common transforming LDPC. The transforming LDPC will be linked to an FP3 pipeline transformer combining the required transformers.

For each RDF document, we use the contained titles of books related to the core concept, and the DBpedia abstract for the core concept, as the input for NER analysis.

#####NER for LKC - Implementation

Stanbol supports a number of NLP/NER frameworks including Stanford NLP and OpenNLP. For this use case, OpenNLP will be used as it provides a basic NER model that can detect Persons, Organizations and Locations in German language texts. While there is also a German model for Stanford NLP this is only available for an older software version that is not compatible with the Stanford/Stanbol integration. As their are also English books to be processed OpenNLP will be configured with both German and English language models.

In common with most of the other Stanbol enhancement engines, OpenNLP accepts plain text. The [Literal Extraction Transformer]( https://github.com/fusepoolP3/p3-literal-extraction-transformer) will provide the plain text input to OpenNLP, by combining the Literal Extraction and Stanbol transformers in a pipeline. The Literal Extraction Transformer can be configured to iterate over specific literal predicates in particular languages, concatenating the text for the same language and emitting plain text.

#####NER for LKC - Expectations

Public available NER models typically include support for Persons, Organizations and Places and are trained based on news text corpora. In-domain (meaning when applied to news texts other than the training set) those systems typically reach F1 measures above 90%. However when applied to other type of texts the performance can drop drastically.

Expected input texts for the LKC domain will be very different from news texts. Texts are essentially concatenated titles and sub titles. Meaning that the texts will not contain full sentences nor will sentences be connected to previous sentences.

Because of that we expect available NER models to perform very badly on the data of this use case. Nevertheless, in spite of reservations about the quality of expected NER results,  we believe the use case is fit for purpose for validating that the platform can handle large amounts of data.

To obtain better results, it would be necessary to train specific NER models based on a training set on the LKC texts. Experience on other datasets suggests that one can train a reasonable good model with about 3000 manually annotated entities per entity type. We consider training an LKC-specific model to be outside the scope of this validation task.

Extracted Named Entities will be represented in the results by using the [Entity Mention Annotation](https://github.com/fusepoolP3/overall-architecture/blob/master/wp3/fp-anno-model/fp-anno-model.md#entity-mention-annotation) as defined by the Fusepool Annotation Model (FAM). The [Literal Extraction Transformer]( https://github.com/fusepoolP3/p3-literal-extraction-transformer) consumes those annotations and converts them to explicit triples to be added to the enriched LKC dataset.

<a name="UseCaseTaskDetails7"></a> 
####7: Match thesauri concepts to each GND

In this step, we will perform *entity linking* against each GND ID with the aim to detect mentions of GND IDs in the titles and sub-titles of the LKC data. Mentioned GND Ids will be represented in the results by using [Entity Annotation](https://github.com/fusepoolP3/overall-architecture/blob/master/wp3/fp-anno-model/fp-anno-model.md#entity-annotation) as defined by the Fusepool Annotation Model (FAM).

NOTE: this is different from the *Named Entity Recognition* in step 6. *NER* detects types of Entities in the text without the need of any controlled vocabulary. This means that NER can detect new/unknown entities. *Entity linking* works based on a controlled vocabulary - in that case the GND. It uses the names (preferred and alternate) as input and looks for mentions of those in the text. If it finds such a mention is links the according entity with that mention in the text.

The Fusepool platform provides several entity linking implementations. Options include:
 
 * Dictionary Matching Transformer,
 * Stanbol Entityhub Linking Engine
 * Stanbol FST Linking Engine 

For this use case we will use the [FST Linking egine](http://stanbol.apache.org/docs/trunk/components/enhancer/engines/lucenefstlinking) of Apache Stanbol as this one is most efficient for linking against vocabularies with the size of the GND. The index required by the FST Linking Engine will be build in a pre-processing step by using the RDF indexing tool provided by Apache Stanbol.

As with step 6, the Literal Extraction Transformer will be used to collect and concatenate the input text from literals of processed Entities. The Literal Extraction Transformer will also be used to consume the extraction results represented by the Fusepool Annotation Model (FAM) and convert them to explicit triples to be added to the enriched LKC data.

In addition to GND IDs Entity Linking will be also provided for the following three SKOS based thesauri:

 * The [Thesaurus for the Social Sciences](http://www.gesis.org/en/services/research/thesauri-und-klassifikationen/social-science-thesaurus/) ([The Soz](http://www.semantic-web-journal.net/sites/default/files/swj279_2.pdf)).
 * The [IPTC](https://iptc.org/) media topics [thesaurus](https://iptc.org/standards/media-topics/), a 1100-term taxonomy with a focus on categorizing text. The Media Topics vocabulary can be viewed on the [IPTC Controlled Vocabulary server]( http://cv.iptc.org/newscodes/mediatopic).
 * The [STW Thesaurus for Economics](http://zbw.eu/stw/versions/latest/download/about.en.html).

The Thesaurus for the Social Sciences (theSoz) is curated by [GESIS](http://www.gesis.org/en/institute/), the largest infrastructure institution for the Social Sciences in Germany (Leibniz Institute for Social Sciences)[1]. GESIS reviews and describes a vast amount of publications from major journals in the Social Sciences, in order to make research retrievable at a central hub. TheSoz is used to describe the contents of scientific efforts with a controlled language (Thesaurus). The list of keywords contains about 12,000 entries, of which more than 8,000 are descriptors (authorised keywords) and about 4,000 non-descriptors (synonyms). All major topics in the social science disciplines are included in three european languages (german, french, english).
 

<a name="UseCaseTaskDetails8"></a> 
####8: Matching against DBpedia

Some entities are not covered by the GND, or the subject terminologies. In order to find additional related concepts in DBpedia and related Wiki pages in Wikipedia, we propose trying to find them using alternative entity linking services. Two candidate services are [dataTXT](http://dandelion.eu/datatxt/) and [DBpedia Spotlight](https://github.com/dbpedia-spotlight/dbpedia-spotlight/wiki). Both are available as Stanbol enhancement engines, accessible through the FP3 Stanbol Enhancer Transformer.

dataTXT is a named entity extraction & linking service that, given a plain text, gives back a set of entities found in the text and links to corresponding entries in Wikipedia. It also optionally returns the entity type from DBpedia. 

dataTXT performs very well even on short texts, on which many other similar services do not. It is based on co-references of entities, it does not use any NLP feature. Even if the input texts are not complete sentences, we expect it to return valid results. For this reason it is our preferred service for this step.

DBpedia Spotlight provides a fallback option. However, we feel it is unlikely to produce better results than dataTXT. Should we opt to try this option, we will configure our own DBpedia Spotlight service.

<a name="DataConsumption"></a> 
##Data Consumption

The large scale validation should include examples of consuming the enriched/interlinked data. For this project, we believe it is sufficient to provide example SPARQL queries illustrating how the Linked Data might be consumed, or at most create a simple user interface.

We intend to provide sample SPARQL queries once the large scale data transformation and enrichment is complete.

<a name="ValidationMetrics"></a> 
##Validation Metrics

###Interlinking

The success metrics described in the DoW for milestone M6, of which Deliverable D5.4 is a part, stipulate "90% of the data stored by the project are interlinked correctly between each other and with public datasets from the Linked Open Data Cloud". To this end, the LKC use case will measure the degree of interlinking achieved between the GND, B3Kat and DBpedia datasets.

In addition to interlinking, we propose assessing the platform's operation in terms of disk, memory and network performance.

### Disk IO / Memory Usage

We will monitor disk IO and memory usage during processing runs in order to see what resources are used. The Linux tools for performing this monitoring have still be assessed and selected.

### Network Performance

In order to properly analyse network performance, we need mechanisms to monitor HTTP requests to the transformer pipeline and individual transformers.

Options being considered include:

**Logging**

The Transforming LDP Proxy could log requests/responses to/from transformers. This should suffice when calling individual transformers, but not for transformers invoked through the pipeline transformer. However, also having the pipeline transformer log requests/responses to all transformers it calls obviates the need for each transformer to perform its own logging.

**Packet Capture**

We propose augmenting logging with network packet capture using tools such as the [pcap](https://en.wikipedia.org/wiki/Pcap) library, the basis of the [tcpdump](https://en.wikipedia.org/wiki/Tcpdump) packet analyser.

Numerous network performance tools have adopted the [HAR](https://www.igvita.com/2012/08/28/web-performance-power-tool-http-archive-har/) (HTTP Archive) [specification](http://www.softwareishard.com/blog/har-12-spec/) from Google as a common data format. We aim to use this data format as the basis for further analysis of the collected network performance data. Export to HAR is built into Chrome for example. Tools exist to generate HAR from `pcap` files, for example [pcap2har](https://github.com/andrewf/pcap2har). Frontend HAR viewer tools include [harviewer](https://code.google.com/p/harviewer/) or [PCAP Web Performance Analyzer](https://pcapperf.appspot.com/). If these tools do not provide the information we need (probably mainly response times), we will have to create our own scripts to analyse the results.

Another potentially useful tool is [YSlow](http://yslow.org/node-server/), an web page performance analyser written in Node.js.

<a name="ValidationResults"></a> 
## Validation Results

The validation results will be provided once available.

<a name="ConclusionsandFutureWork"></a> 
## Conclusions and Future Work

**Status, June 2015:** At the current time this is an **interim report** detailing our goals and approach to large scale technical validation of the FP3 Linked Data Platform. The validation work is currently on-going.  Regrettably, we cannot draw any conclusions at this stage, but will revise this section once we are in a position to do so.

<a name="AppendixA"></a> 
## Appendix A: GND RDF Description of Hanns Eisler

Below is the description of Hanns Eisler extracted from the GND dataset, illustrating a typical GND RDF description.

    <rdf:Description rdf:about="http://d-nb.info/gnd/118529692">
        <rdf:type rdf:resource="http://d-nb.info/standards/elementset/gnd#DifferentiatedPerson" />
        <foaf:page rdf:resource="http://de.wikipedia.org/wiki/Hanns_Eisler" />
        <owl:sameAs rdf:resource="http://dbpedia.org/resource/Hanns_Eisler" />
        <owl:sameAs rdf:resource="http://viaf.org/viaf/19865132" />
        <gndo:gndIdentifier>118529692</gndo:gndIdentifier>
        <gndo:oldAuthorityNumber>(DE-588a)118529692</gndo:oldAuthorityNumber>
        <gndo:oldAuthorityNumber>(DE-588a)185686249</gndo:oldAuthorityNumber>
        <gndo:oldAuthorityNumber>(DE-588a)139523375</gndo:oldAuthorityNumber>
        <gndo:oldAuthorityNumber>(DE-588a)134366263</gndo:oldAuthorityNumber>
        <gndo:oldAuthorityNumber>(DE-101c)310056268</gndo:oldAuthorityNumber>
        <gndo:oldAuthorityNumber>(DE-588c)4014114-7</gndo:oldAuthorityNumber>
        <gndo:variantNameForThePerson>Eisler, ...</gndo:variantNameForThePerson>
        <gndo:variantNameEntityForThePerson rdf:parseType="Resource">
            <gndo:forename>...</gndo:forename>
            <gndo:surname>Eisler</gndo:surname>
        </gndo:variantNameEntityForThePerson>
        <gndo:variantNameForThePerson>Eissler, Hanns</gndo:variantNameForThePerson>
        <gndo:variantNameEntityForThePerson rdf:parseType="Resource">
            <gndo:forename>Hanns</gndo:forename>
            <gndo:surname>Eissler</gndo:surname>
        </gndo:variantNameEntityForThePerson>
        <gndo:variantNameForThePerson>Eisler, Hans</gndo:variantNameForThePerson>
        <gndo:variantNameEntityForThePerson rdf:parseType="Resource">
            <gndo:forename>Hans</gndo:forename>
            <gndo:surname>Eisler</gndo:surname>
        </gndo:variantNameEntityForThePerson>
        <gndo:variantNameForThePerson>Eisler, Johannes</gndo:variantNameForThePerson>
        <gndo:variantNameEntityForThePerson rdf:parseType="Resource">
            <gndo:forename>Johannes</gndo:forename>
            <gndo:surname>Eisler</gndo:surname>
        </gndo:variantNameEntityForThePerson>
        <gndo:preferredNameForThePerson>Eisler, Hanns</gndo:preferredNameForThePerson>
        <gndo:preferredNameEntityForThePerson rdf:parseType="Resource">
            <gndo:forename>Hanns</gndo:forename>
            <gndo:surname>Eisler</gndo:surname>
        </gndo:preferredNameEntityForThePerson>
        <gndo:familialRelationship rdf:resource="http://d-nb.info/gnd/116435410" />
        <gndo:familialRelationship rdf:resource="http://d-nb.info/gnd/124362214" />
        <gndo:familialRelationship rdf:resource="http://d-nb.info/gnd/118691392" />
        <gndo:familialRelationship rdf:resource="http://d-nb.info/gnd/118681850" />
        <gndo:familialRelationship rdf:resource="http://d-nb.info/gnd/1054173877" />
        <gndo:professionOrOccupation rdf:resource="http://d-nb.info/gnd/4032009-1" />
        <gndo:professionOrOccupation rdf:resource="http://d-nb.info/gnd/4040841-3" />
        <gndo:playedInstrument rdf:resource="http://d-nb.info/gnd/4030982-4" />
        <gndo:playedInstrument rdf:resource="http://d-nb.info/gnd/4057587-1" />
        <gndo:gndSubjectCategory rdf:resource="http://d-nb.info/standards/vocab/gnd/gnd-sc#14.4p" />
        <gndo:geographicAreaCode rdf:resource="http://d-nb.info/standards/vocab/gnd/geographic-area-code#XA-DE" />
        <gndo:geographicAreaCode rdf:resource="http://d-nb.info/standards/vocab/gnd/geographic-area-code#XD-US" />
        <gndo:geographicAreaCode rdf:resource="http://d-nb.info/standards/vocab/gnd/geographic-area-code#XA-AT" />
        <gndo:placeOfBirth rdf:resource="http://d-nb.info/gnd/4035206-7" />
        <gndo:placeOfDeath rdf:resource="http://d-nb.info/gnd/4005728-8" />
        <gndo:placeOfActivity rdf:resource="http://d-nb.info/gnd/4042011-5" />
        <gndo:placeOfExile rdf:resource="http://d-nb.info/gnd/4078704-7" />
        <owl:sameAs rdf:resource="http://www.filmportal.de/person/18AC4FE0900B4565A8D821ED5F6A175E" />
        <gndo:gender rdf:resource="http://d-nb.info/standards/vocab/gnd/Gender#male" />
        <gndo:dateOfBirth rdf:datatype="http://www.w3.org/2001/XMLSchema#date">1898-07-06</gndo:dateOfBirth>
        <gndo:dateOfDeath rdf:datatype="http://www.w3.org/2001/XMLSchema#date">1962-09-06</gndo:dateOfDeath>
    </rdf:Description>

<a name="AppendixB"></a> 
## Appendix B: Overview Notes on Apache Stanbol

This appendix provides a brief overview of the main components of Apache Stanbol relevant to the LKC use case, with the aim of supplying some background context to the technical points raised in the main document, for readers unfamiliar with Stanbol.

<a name="AppendixBLELUC"></a> 
### Information Extraction for the LKC Use Case

For the LKC use case the following information extraction workflow is expected

![Information Extraction Workflow](d5.4-info-extraction-workflow.png)

The [Literal Extraction Transformer](https://github.com/fusepoolP3/p3-literal-extraction-transformer) is used to collect literals of the source RDF data to be used for information extraction. It also processes the information extraction results encoded using the [Fusepool Annotation Model](https://github.com/fusepoolP3/overall-architecture/blob/master/wp3/fp-anno-model/fp-anno-model.md) (FAM) and converts them to simple statements for the enriched RDF data.

The Literal Extraction Transformer is configured with a second transformer doing the actual information extraction work. As for the LKC use case Apache Stanbol is used for the information extraction work the [Stanbol Enhancer Transformer](https://github.com/fusepoolP3/p3-stanbol-enhancer-adapter/tree/master/service) is configured. This "transformer adapter" for the Apache Stanbol Enhancer allows to use Stanbol Enhancement Chains or single Enhancement Engines as Fusepool Transformers.

As the LKC use case requires several information extraction capabilities a Enhancement Chain supporting (1) Named Entity Linking , (2) Entity Linking against the GND, GESIS, STW and IPTC and (3) DBpedia linking by using the DataTXT engine is configured. As Stanbol uses the FISE annotation model also an engine that converts FISE into the Fusepool Annotation Model is required.

All the transformers and Stanbol components mentioned above are described in detail in D3.1: Section 9 provides details about the Fusepool Annotation Model; Section 10.1 gives details about the integration of Apache Stanbol to the Fusepool Plattform as Transformer. Section 10.2 provides details about the DataTXT integration.

The remainder of this appendix aims to provide a short overview on Apache Stanbol components used for the LKC use case.

<a name="AppendixBEC"></a> 
### Enhancement Chain

An [Enhancement Chain](http://stanbol.apache.org/docs/trunk/components/enhancer/chains/) defines how content parsed to the Stanbol Enhancer is processed. More concretely it defines which [Enhancement Engines](http://stanbol.apache.org/docs/trunk/components/enhancer/engines) and in what order are used to process the parsed content. For the LKC use case an Enhancement chain with all the required Information Extraction Capabilities needs to be configured.

A typical Stanbol enhancement workflow is depicted below:

![Typical Stanbol enhancement workflow](https://stanbol.apache.org/docs/trunk/enhancementworkflow.png)

The typical information extraction workflow used with Apache Stanbol starts with a pre-processing step. In this step the parsed content is prepared for later information extraction. Plain Text extraction from rich text documents is a typical task performed in this step. For the LKC use case this step is not required as this work is already done by the [Literal Extraction Transformer](https://github.com/fusepoolP3/p3-literal-extraction-transformer) outside of Apache Stanbol.

The second step is about natural language processing. For the LKC this includes Language Detection and Named Entity Recognition. As OpenNLP requires Sentence Detection and Tokenization before NER those need also to be configured.

In the Semantic Lifting step the Entity Linking against the four controlled vocabularies and the dataTXT linking to DBPedia will take place.

In the post processing phase the FISE enhancements will be refactored to the Fusepool Annotation Model (FAM).

Apache Stanbol provides different Enhancement Chain implementations. Typically users want to use the [Weighted Chain](http://stanbol.apache.org/docs/trunk/components/enhancer/chains/weightedchain) as this supports automatic ordering of configured enhancement engines. So for the user it is sufficient to provide the list of required engines. The ordering is not of importance.

<a name="AppendixBEE"></a> 
### Enhancement Engines

An Enhancement Engine is an information extraction component. It can extract plain text from a PDF file; detect the language of a text of a chapter; extract Named Entities like Persons or Placed, detect Mentions of Entities managed in a Controlled vocabulary; disambiguate results of previous engines or just refactor annotations to a different annotation model. 

Apache Stanbol comes with a wide range of Enhancement Engines and users can also implement and use customs one. For the LKC use case the following engines are required:

* Language Detection: The most common Language detection engine for Apache Stanbol is the [Langdetect Engine](http://stanbol.apache.org/docs/trunk/components/enhancer/engines/langdetectengine)
* Named Entity Recognition: For the use case [Apache OpenNLP](http://opennlp.apache.org/) will be used. For Named Entity Recognition the OpenNLP [Sentence Detection](http://stanbol.apache.org/docs/trunk/components/enhancer/engines/opennlpsentence), [Tokenizer](http://stanbol.apache.org/docs/trunk/components/enhancer/engines/opennlptokenizer) and [NER](http://stanbol.apache.org/docs/trunk/components/enhancer/engines/opennlpner) engines will get used.
* Entity Linking: As the texts for the LKC are not full sentences a linking engine that can link against all words in the text is best suited (PLAIN linking mode). For such a scenario the [FST Linking Engine](http://stanbol.apache.org/docs/trunk/components/enhancer/engines/lucenefstlinking) is best suited. It is based on The Apache Lucene FST (Finit State Transducer) API witch allows it to hold labels of large vocabularies fully in memory (e.g. for all English labels of DBPedia one needs less as 300MByte of memory). 
* DBPedia Linking: While the FST linking engine could also be used for DBPedia it does not provide support for disambiguation. Because of the we will use the [DataTXT engine](https://github.com/fusepoolP3/p3-datatxt-stanbol) instead. This engine is based on the [DataTXT](https://dandelion.eu/products/datatxt/) service.
* Fusepool Annotation Model support: Apache Stanbol uses the [Stanbol Enhancement Structure](http://stanbol.apache.org/docs/trunk/components/enhancer/enhancementstructure). When used as Fusepool Transformer those annotations need to be converted to the Fusepool Annotation Model. The [FAM engine](https://github.com/fusepoolP3/p3-stanbol-engine-fam) implements this conversion.

<a name="AppendixBWCV"></a> 
### Working with Custom Vocabularies

For using Entity Linking Engines it is required to create special indexes over custom vocabularies. For small and medium sized vocabularies those can be created while uploading them to Apache Stanbol. Hoever for big vocabularies or if one want to have more control on how those indexes are build Apache Stanbol provides a special batch processing tool for creating those.

The whole process is described by the [Working with Custom Vocabularies](https://stanbol.apache.org/docs/trunk/customvocabulary.html) usage scenario.

Here are the main Steps of the process:

* Build the Entityhub Indexing Tool and grab the binary `org.apache.stanbol.entityhub.indexing.genericrdf-*-jar-with-dependencies.jar`
* call the binary with the `init` command to initialise the configuration hierarchy
* edit the `indexing/conig/indexing.properties` file. See comments in this file for more information
* copy the RDF files of your controlled vocabulary to the `indexing/resources/rdfdata` folder
* call the binary with the `index` command to index the vocabulary
* after the indexing finishes the `indexing/dist` folder will contain two files
    1. `org.apache.stanbol.data.site.{name}-{version}.jar` an OSGI bundle to be installed to the Stanbol runtime. This Bundle provides the Entityhub configuration for the indexed dataset
    2. `{name}.solrindex.zip` an archived Solr core. This needs to be installed as data file to apache stanbol. The easiest way to do this is to copy it to the `stanbol/datafiles` folder of your stanbol instance.
    
After performing those steps you will have an Entityhub Site holding your Custom Vocabulary ready to be used with any of the Entity Linking Engines provided by Apache Stanbol. 

<a name="AppendixBSEH"></a> 
### Stanbol EntityHub

The [Entityhub](https://stanbol.apache.org/docs/trunk/components/entityhub) is the Stanbol component responsible for providing the information about entities relevant to the users domain.  For the LKC use case it will be used to manage the indexes of the Vocabulary used for Entity Linking. Those vocabularies will include

* The [[GND catalog of the German National Library](http://www.dnb.de/EN/Standardisierung/GND/gnd.html)
* The [Thesaurus for the Social Sciences](http://www.gesis.org/en/services/research/thesauri-und-klassifikationen/social-science-thesaurus/) ([The Soz](http://www.semantic-web-journal.net/sites/default/files/swj279_2.pdf)).
* The [IPTC](https://iptc.org/) media topics [thesaurus](https://iptc.org/standards/media-topics/), a 1100-term taxonomy with a focus on categorizing text. The Media Topics vocabulary can be viewed on the [IPTC Controlled Vocabulary server]( http://cv.iptc.org/newscodes/mediatopic).
* The [STW Thesaurus for Economics](http://zbw.eu/stw/versions/latest/download/about.en.html).

All those indexes will be build using the Entityhub Indexing Tool and afterwards be installed to the Stanbol Runtime.

<a name="AppendixC"></a> 
## Appendix C: Dataset Sources

This appendix lists the used data sources which are used for Large Scale validation. The sources were fetched on `{TODO} add date when sources are fetched` and loaded into Virtuoso.

<a name="AppendixCB3Kat"></a> 
### B3Kat 
 
* http://lod.b3kat.de/download/lod.b3kat.de.part0.ttl.gz
* http://lod.b3kat.de/download/lod.b3kat.de.part1.ttl.gz
* http://lod.b3kat.de/download/lod.b3kat.de.part2.ttl.gz
* http://lod.b3kat.de/download/lod.b3kat.de.part3.ttl.gz
* http://lod.b3kat.de/download/lod.b3kat.de.part4.ttl.gz
* http://lod.b3kat.de/download/lod.b3kat.de.part5.ttl.gz
* http://lod.b3kat.de/download/lod.b3kat.de.part6.ttl.gz
* http://lod.b3kat.de/download/lod.b3kat.de.part7.ttl.gz
* http://lod.b3kat.de/download/lod.b3kat.de.part8.ttl.gz
* http://lod.b3kat.de/download/lod.b3kat.de.part9.ttl.gz
* http://lod.b3kat.de/download/lod.b3kat.de.part10.ttl.gz
* http://lod.b3kat.de/download/lod.b3kat.de.part11.ttl.gz
* http://lod.b3kat.de/download/lod.b3kat.de.part12.ttl.gz
* http://lod.b3kat.de/download/lod.b3kat.de.part13.ttl.gz
* http://lod.b3kat.de/download/lod.b3kat.de.part14.ttl.gz
* http://lod.b3kat.de/download/lod.b3kat.de.part15.ttl.gz
* http://lod.b3kat.de/download/lod.b3kat.de.part16.ttl.gz
* http://lod.b3kat.de/download/lod.b3kat.de.part17.ttl.gz
* http://lod.b3kat.de/download/lod.b3kat.de.part18.ttl.gz
* http://lod.b3kat.de/download/lod.b3kat.de.part19.ttl.gz
* http://lod.b3kat.de/download/lod.b3kat.de.part20.ttl.gz
* http://lod.b3kat.de/download/lod.b3kat.de.part21.ttl.gz
* http://lod.b3kat.de/download/lod.b3kat.de.part22.ttl.gz
* http://lod.b3kat.de/download/lod.b3kat.de.part23.ttl.gz
* http://lod.b3kat.de/download/lod.b3kat.de.part24.ttl.gz
* http://lod.b3kat.de/download/lod.b3kat.de.part25.ttl.gz
* http://lod.b3kat.de/download/lod.b3kat.de.part26.ttl.gz
* http://lod.b3kat.de/download/lod.b3kat.de.part27.ttl.gz

<a name="AppendixCGND"></a> 
### GND

* [Entry page for downloads](http://datendienst.dnb.de/cgi-bin/mabit.pl?userID=opendata&pass=opendata&cmd=login)
* [GND Concepts](http://datendienst.dnb.de/cgi-bin/mabit.pl?cmd=fetch&userID=opendata&pass=opendata&mabheft=GND.ttl.gz)
* [GND Titles](http://datendienst.dnb.de/cgi-bin/mabit.pl?cmd=fetch&userID=opendata&pass=opendata&mabheft=DNBTitel.ttl.gz)

<a name="AppendixCSTW"></a> 
### STW

* [Entry page](http://zbw.eu/stw/versions/8.10/download/about.de.html)
* [STW v 8.10 (SKOS)](http://zbw.eu/stw/versions/8.10/download/stw.ttl.zip)
* [Mapping SWD - STW (SKOS)](http://zbw.eu/stw/versions/8.10/download/stw_gnd_mapping.ttl.zip)
* [Mapping STW - DBpedia (SKOS)](http://zbw.eu/stw/versions/8.10/download/stw_dbpedia_mapping.ttl.zip)
* [Mapping TheSoz - STW (SKOS)](http://zbw.eu/stw/versions/8.10/download/stw_thesoz_mapping.ttl.zip)

<a name="AppendixCTheSoz"></a> 
### TheSoz

* [Entry page](http://www.gesis.org/en/services/research/thesauri-und-klassifikationen/social-science-thesaurus/)
* [Direct download](http://www.etracker.de/lnkcnt.php?et=qPKGYV&url=http://www.gesis.org/fileadmin/upload/dienstleistung/tools_standards/thesoz_skos_turtle.zip&lnkname=fileadmin/upload/dienstleistung/tools_standards/thesoz_skos_turtle.zip)

The Turtle file in this thesaurus is not correct Turtle and parsers might fail on it. We load a cleaned Turtle file.

<a name="AppendixCIPTC"></a> 
### IPTC

The files need to be fetched with content negotiation from the [IPTC Server](http://dev.iptc.org/NewsCodes-CV-Server#glformatlang)

    curl -X GET -H "Accept:text/turtle" -H "Accept-Language: de" http://cv.iptc.org/newscodes/mediatopic/ > iptc-de.ttl
    curl -X GET -H "Accept:text/turtle" -H "Accept-Language: en" http://cv.iptc.org/newscodes/mediatopic/ > iptc-en.ttl

<a name="AppendixCEuroVoc"></a> 
### EuroVoc

* [EuroVoc in SKOS](http://publications.europa.eu/mdr/resource/thesaurus/eurovoc/skos/eurovoc_skos.zip)

